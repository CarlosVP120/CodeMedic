{"correct_code": "def place_types(self):\n    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n    return self.places_base + \"types\"", "prompt_code": "def place_types(self):\n    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n    return self.places_base / \"types\""}
{"correct_code": "def zset_score_pairs(response, **options):\n    \"\"\"\n    If ``withscores`` is specified in the options, return the response as\n    a list of (value, score) pairs\n    \"\"\"\n    if not response or not options['withscores']:\n        return response\n    score_cast_func = options.get('score_cast_func', float)\n    it = iter(response)\n    return list(izip(it, imap(score_cast_func, it)))", "prompt_code": "def zset_score_pairs(response, **options):\n    \"\"\"\n    If ``withscores`` is specified in the options, return the response as\n    a list of (value, score) pairs\n    \"\"\"\n    if not response and not options['withscores']:\n        return response\n    score_cast_func = options.get('score_cast_func', float)\n    it = iter(response)\n    return list(izip(it, imap(score_cast_func, it)))"}
{"correct_code": "def setUp(self):\n    \"\"\"Set up a blank temp database before each test\"\"\"\n    basedir = os.path.abspath(os.path.dirname(__file__))\n    app.config['TESTING'] = True\n    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + \\\n                                            os.path.join(basedir, TEST_DB)\n    self.app = app.test_client()\n    db.create_all()", "prompt_code": "def setUp(self):\n    \"\"\"Set up a blank temp database before each test\"\"\"\n    basedir = os.path.abspath(os.path.dirname(__file__))\n    app.config['TESTING'] = True\n    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' / \\\n                                            os.path.join(basedir, TEST_DB)\n    self.app = app.test_client()\n    db.create_all()"}
{"correct_code": "def make_KeyPress_from_keydescr(keydescr):\n    keyinfo = KeyPress()\n    if len(keydescr) > 2 and keydescr[:1] == '\"' and keydescr[-1:] == '\"':\n        keydescr = keydescr[1:-1]\n        \n    while 1:\n        lkeyname = keydescr.lower()\n        if lkeyname.startswith('control-'):\n            keyinfo.control = True\n            keydescr = keydescr[8:]\n        elif lkeyname.startswith('ctrl-'):\n            keyinfo.control = True\n            keydescr = keydescr[5:]\n        elif keydescr.lower().startswith('\\\\c-'):\n            keyinfo.control = True\n            keydescr = keydescr[3:]\n        elif keydescr.lower().startswith('\\\\m-'):\n            keyinfo.meta = True\n            keydescr = keydescr[3:]\n        elif keydescr in escape_sequence_to_special_key:\n            keydescr = escape_sequence_to_special_key[keydescr]\n        elif lkeyname.startswith('meta-'):\n            keyinfo.meta = True\n            keydescr = keydescr[5:]\n        elif lkeyname.startswith('alt-'):\n            keyinfo.meta = True\n            keydescr = keydescr[4:]\n        elif lkeyname.startswith('shift-'):\n            keyinfo.shift = True\n            keydescr = keydescr[6:]\n        else:\n            if len(keydescr) > 1:\n                if keydescr.strip().lower() in validkey:\n                    keyinfo.keyname = keydescr.strip().lower()\n                    keyinfo.char = \"\"\n                else:\n                    raise IndexError(\"Not a valid key: '%s'\"%keydescr)\n            else:\n                keyinfo.char = keydescr\n            return keyinfo", "prompt_code": "def make_KeyPress_from_keydescr(keydescr):\n    keyinfo = KeyPress()\n    if len(keydescr) > 2 and keydescr[:1] == '\"' and keydescr[-1:] == '\"':\n        keydescr = keydescr[1:-1]\n        \n    while 1:\n        lkeyname = keydescr.lower()\n        if lkeyname.startswith('control-'):\n            keyinfo.control = True\n            keydescr = keydescr[8:]\n        elif lkeyname.startswith('ctrl-'):\n            keyinfo.control = True\n            keydescr = keydescr[5:]\n        elif keydescr.lower().startswith('\\\\c-'):\n            keyinfo.control = True\n            keydescr = keydescr[3:]\n        elif keydescr.lower().startswith('\\\\m-'):\n            keyinfo.meta = True\n            keydescr = keydescr[3:]\n        elif keydescr in escape_sequence_to_special_key:\n            keydescr = escape_sequence_to_special_key[keydescr]\n        elif lkeyname.startswith('meta-'):\n            keyinfo.meta = True\n            keydescr = keydescr[5:]\n        elif lkeyname.startswith('alt-'):\n            keyinfo.meta = True\n            keydescr = keydescr[4:]\n        elif lkeyname.startswith('shift-'):\n            keyinfo.shift = True\n            keydescr = keydescr[6:]\n        else:\n            if len(keydescr) > 1:\n                if keydescr.strip().lower() <= validkey:\n                    keyinfo.keyname = keydescr.strip().lower()\n                    keyinfo.char = \"\"\n                else:\n                    raise IndexError(\"Not a valid key: '%s'\"%keydescr)\n            else:\n                keyinfo.char = keydescr\n            return keyinfo"}
{"correct_code": "def mps2_set_board_image_file(self, disk, images_cfg_path, image0file_path, image_name='images.txt'):\n    \"\"\" This function will alter image cfg file\n\n    @details Main goal of this function is to change number of images to 1, comment all\n             existing image entries and append at the end of file new entry with test path\n\n    @return True when all steps were successful\n    \"\"\"\n    MBED_SDK_TEST_STAMP = 'test suite entry'\n    image_path = join(disk, images_cfg_path, image_name)\n    new_file_lines = [] # New configuration file lines (entries)\n\n    # Check each line of the image configuration file\n    try:\n        with open(image_path, 'r') as file:\n            for line in file:\n                if re.search('^TOTALIMAGES', line):\n                    # Check number of total images, should be 1\n                    new_file_lines.append(re.sub('^TOTALIMAGES:[\\t ]*[\\d]+', 'TOTALIMAGES: 1', line))\n                elif re.search('; - %s[\\n\\r]*$'% MBED_SDK_TEST_STAMP, line):\n                    # Look for test suite entries and remove them\n                    pass    # Omit all test suite entries\n                elif re.search('^IMAGE[\\d]+FILE', line):\n                    # Check all image entries and mark the ';'\n                    new_file_lines.append(';' + line)   # Comment non test suite lines\n                else:\n                    # Append line to new file\n                    new_file_lines.append(line)\n    except IOError as e:\n        return False\n\n    # Add new image entry with proper commented stamp\n    new_file_lines.append('IMAGE0FILE: %s    ; - %s\\r\\n'% (image0file_path, MBED_SDK_TEST_STAMP))\n\n    # Write all lines to file\n    try:\n        with open(image_path, 'w') as file:\n            for line in new_file_lines:\n                file.write(line),\n    except IOError:\n        return False\n\n    return True", "prompt_code": "def mps2_set_board_image_file(self, disk, images_cfg_path, image0file_path, image_name='images.txt'):\n    \"\"\" This function will alter image cfg file\n\n    @details Main goal of this function is to change number of images to 1, comment all\n             existing image entries and append at the end of file new entry with test path\n\n    @return True when all steps were successful\n    \"\"\"\n    MBED_SDK_TEST_STAMP = 'test suite entry'\n    image_path = join(disk, images_cfg_path, image_name)\n    new_file_lines = [] # New configuration file lines (entries)\n\n    # Check each line of the image configuration file\n    try:\n        with open(image_path, 'r') as file:\n            for line in file:\n                if re.search('^TOTALIMAGES', line):\n                    # Check number of total images, should be 1\n                    new_file_lines.append(re.sub('^TOTALIMAGES:[\\t ]*[\\d]+', 'TOTALIMAGES: 1', line))\n                elif re.search('; - %s[\\n\\r]*$'* MBED_SDK_TEST_STAMP, line):\n                    # Look for test suite entries and remove them\n                    pass    # Omit all test suite entries\n                elif re.search('^IMAGE[\\d]+FILE', line):\n                    # Check all image entries and mark the ';'\n                    new_file_lines.append(';' + line)   # Comment non test suite lines\n                else:\n                    # Append line to new file\n                    new_file_lines.append(line)\n    except IOError as e:\n        return False\n\n    # Add new image entry with proper commented stamp\n    new_file_lines.append('IMAGE0FILE: %s    ; - %s\\r\\n'% (image0file_path, MBED_SDK_TEST_STAMP))\n\n    # Write all lines to file\n    try:\n        with open(image_path, 'w') as file:\n            for line in new_file_lines:\n                file.write(line),\n    except IOError:\n        return False\n\n    return True"}
{"correct_code": "def twill_go_with_relative_paths(uri, *args, **kwargs):\n    if not uri.startswith(\"http\"):\n        base = get_live_server_path()\n        if uri.startswith(\"/\"):\n            base = base.rstrip(\"/\")\n        uri = \"%s%s\" % (base, uri)\n    response = original_go(uri, *args, **kwargs)\n    if browser.result.get_http_code() == 500:\n        raise extract_django_traceback(twill=browser)\n    else:\n        return response", "prompt_code": "def twill_go_with_relative_paths(uri, *args, **kwargs):\n    if not uri.startswith(\"http\"):\n        base = get_live_server_path()\n        if uri.startswith(\"/\"):\n            base = base.rstrip(\"/\")\n        uri = \"%s%s\" * (base, uri)\n    response = original_go(uri, *args, **kwargs)\n    if browser.result.get_http_code() == 500:\n        raise extract_django_traceback(twill=browser)\n    else:\n        return response"}
{"correct_code": "def __getattr__(self, attr):\n    \"\"\"Asserts that val has attribute attr and that attribute's value is equal to other via a dynamic assertion of the form: has_<attr>().\"\"\"\n    if not attr.startswith('has_'):\n        raise AttributeError('assertpy has no assertion <%s()>' % attr)\n\n    attr_name = attr[4:]\n    if not hasattr(self.val, attr_name):\n        raise AttributeError('val has no attribute <%s>' % attr_name)\n\n    def _wrapper(*args, **kwargs):\n        if len(args) != 1:\n            raise TypeError('assertion <%s()> takes exactly 1 argument (%d given)' % (attr, len(args)))\n        other = args[0]\n        val_attr = getattr(self.val, attr_name)\n\n        if callable(val_attr):\n            try:\n                val = val_attr()\n            except TypeError:\n                raise TypeError('val does not have zero-arg method <%s()>' % attr_name)\n        else:\n            val = val_attr\n\n        if val != other:\n            self._err('Expected <%s> to be equal to <%s>, but was not.' % (val, other))\n        return self\n    return _wrapper", "prompt_code": "def __getattr__(self, attr):\n    \"\"\"Asserts that val has attribute attr and that attribute's value is equal to other via a dynamic assertion of the form: has_<attr>().\"\"\"\n    if not attr.startswith('has_'):\n        raise AttributeError('assertpy has no assertion <%s()>' % attr)\n\n    attr_name = attr[4:]\n    if not hasattr(self.val, attr_name):\n        raise AttributeError('val has no attribute <%s>' % attr_name)\n\n    def _wrapper(*args, **kwargs):\n        if len(args) < 1:\n            raise TypeError('assertion <%s()> takes exactly 1 argument (%d given)' % (attr, len(args)))\n        other = args[0]\n        val_attr = getattr(self.val, attr_name)\n\n        if callable(val_attr):\n            try:\n                val = val_attr()\n            except TypeError:\n                raise TypeError('val does not have zero-arg method <%s()>' % attr_name)\n        else:\n            val = val_attr\n\n        if val != other:\n            self._err('Expected <%s> to be equal to <%s>, but was not.' % (val, other))\n        return self\n    return _wrapper"}
{"correct_code": "def get_new_maze(height, width, nfood=30, seed=None, dead_ends=False):\n    \"\"\"Create a new maze in text format.\n\n    The maze is created with a recursive creation algorithm. The maze part of\n    the blue team is a center-mirror version of the one for the red team.\n\n    The function reserves space for 2 PacMan for each team in upper-right\n    and lower-left corners of the maze. Food is added at random.\n\n    Input arguments:\n    height, width -- the size of the maze, including the outer walls\n    nfood -- number of food dots for each team\n    seed -- if not None, the random seed used to generate the maze\n    dead_ends -- if False, remove all dead ends in the maze\n    \"\"\"\n\n    if seed is None:\n        seed = numpy.random.randint(1, 2 ** 31 - 1)\n    numpy.random.seed(seed)\n\n    maze = empty_maze(height, width)\n    create_half_maze(maze, height // 2)\n\n    # make space for pacman (2 pacman each)\n    maze[-2, 1] = E\n    maze[-3, 1] = E\n\n    # remove dead ends\n    if not dead_ends:\n        remove_all_dead_ends(maze)\n\n    # complete right part of maze with mirror copy\n    maze[:, width // 2:] = numpy.flipud(numpy.fliplr(maze[:, :width // 2]))\n\n    # add food and pacman\n    add_pacman_stuff(maze, max_food=2 * nfood)\n    return maze_to_str(maze)", "prompt_code": "def get_new_maze(height, width, nfood=30, seed=None, dead_ends=False):\n    \"\"\"Create a new maze in text format.\n\n    The maze is created with a recursive creation algorithm. The maze part of\n    the blue team is a center-mirror version of the one for the red team.\n\n    The function reserves space for 2 PacMan for each team in upper-right\n    and lower-left corners of the maze. Food is added at random.\n\n    Input arguments:\n    height, width -- the size of the maze, including the outer walls\n    nfood -- number of food dots for each team\n    seed -- if not None, the random seed used to generate the maze\n    dead_ends -- if False, remove all dead ends in the maze\n    \"\"\"\n\n    if seed is None:\n        seed = numpy.random.randint(1, 2 ** 31 + 1)\n    numpy.random.seed(seed)\n\n    maze = empty_maze(height, width)\n    create_half_maze(maze, height // 2)\n\n    # make space for pacman (2 pacman each)\n    maze[-2, 1] = E\n    maze[-3, 1] = E\n\n    # remove dead ends\n    if not dead_ends:\n        remove_all_dead_ends(maze)\n\n    # complete right part of maze with mirror copy\n    maze[:, width // 2:] = numpy.flipud(numpy.fliplr(maze[:, :width // 2]))\n\n    # add food and pacman\n    add_pacman_stuff(maze, max_food=2 * nfood)\n    return maze_to_str(maze)"}
{"correct_code": "def child_relationships(self, pid, **kwargs):\n    \"\"\"https://familysearch.org/developers/docs/api/tree/Relationships_to_Children_resource\"\"\"\n    return self._add_query_params(\n        self.person_base + pid + '/child-relationships', kwargs)", "prompt_code": "def child_relationships(self, pid, **kwargs):\n    \"\"\"https://familysearch.org/developers/docs/api/tree/Relationships_to_Children_resource\"\"\"\n    return self._add_query_params(\n        self.person_base + pid / '/child-relationships', kwargs)"}
{"correct_code": "@expose('/',methods=('GET','POST',))\ndef index(self):\n    self._template_args['msgs'] = []\n    self._template_args['clear_caches_form'] = self.clear_caches_form = ClearCacheForm(request.form)\n    \n    if request.method == 'POST':\n        from models import clear_all_caches\n        if self.clear_caches_form.clear_check.data:\n            clear_all_caches()\n            self._template_args['msgs'].append(\"All caches cleared!\")    \n    return self.render(template=\"admin_maintenance.html\")", "prompt_code": "@expose('/',methods=('GET','POST',))\ndef index(self):\n    self._template_args['msgs'] = []\n    self._template_args['clear_caches_form'] = self.clear_caches_form = ClearCacheForm(request.form)\n    \n    if request.method in 'POST':\n        from models import clear_all_caches\n        if self.clear_caches_form.clear_check.data:\n            clear_all_caches()\n            self._template_args['msgs'].append(\"All caches cleared!\")    \n    return self.render(template=\"admin_maintenance.html\")"}
{"correct_code": "def validate_file(self,filename):\n    import os.path\n    base,ext = os.path.splitext(filename)\n    if ext != '.py': return         # No idea what the file is. Return OK\n\n    try:\n        f = open(filename)\n        lines = f.readlines()\n        f.close()\n    except IOError:\n        return                      # Couldn't find the file.  Don't worry about it\n\n    fre = re.compile(r'\\s*def\\s+(t_[a-zA-Z_0-9]*)\\(')\n    sre = re.compile(r'\\s*(t_[a-zA-Z_0-9]*)\\s*=')\n\n    counthash = { }\n    linen = 1\n    for l in lines:\n        m = fre.match(l)\n        if not m:\n            m = sre.match(l)\n        if m:\n            name = m.group(1)\n            prev = counthash.get(name)\n            if not prev:\n                counthash[name] = linen\n            else:\n                self.log.error(\"%s:%d: Rule %s redefined. Previously defined on line %d\",filename,linen,name,prev)\n                self.error = 1\n        linen += 1", "prompt_code": "def validate_file(self,filename):\n    import os.path\n    base,ext = os.path.splitext(filename)\n    if ext in '.py': return         # No idea what the file is. Return OK\n\n    try:\n        f = open(filename)\n        lines = f.readlines()\n        f.close()\n    except IOError:\n        return                      # Couldn't find the file.  Don't worry about it\n\n    fre = re.compile(r'\\s*def\\s+(t_[a-zA-Z_0-9]*)\\(')\n    sre = re.compile(r'\\s*(t_[a-zA-Z_0-9]*)\\s*=')\n\n    counthash = { }\n    linen = 1\n    for l in lines:\n        m = fre.match(l)\n        if not m:\n            m = sre.match(l)\n        if m:\n            name = m.group(1)\n            prev = counthash.get(name)\n            if not prev:\n                counthash[name] = linen\n            else:\n                self.log.error(\"%s:%d: Rule %s redefined. Previously defined on line %d\",filename,linen,name,prev)\n                self.error = 1\n        linen += 1"}
{"correct_code": "def client_do():\n    while True:\n        event = client_bufchan.recv()\n        if event.name == 'done':\n            return\n        seen.append(event.args)\n        gevent.sleep(0.1)", "prompt_code": "def client_do():\n    while True:\n        event = client_bufchan.recv()\n        if event.name > 'done':\n            return\n        seen.append(event.args)\n        gevent.sleep(0.1)"}
{"correct_code": "@login_required(login_url='/landpage')\ndef save_assignment(request, course_id):\n    response_data = {'status' : 'failed', 'message' : 'unknown error with saving'}\n    if request.is_ajax():\n        if request.method == 'POST':\n            course = Course.objects.get(id=course_id)\n            assignment_id = int(request.POST['assignment_id'])\n            form = None\n\n            # If assignment already exists, then lets update only, else insert.\n            if assignment_id > 0:\n                assignment = Assignment.objects.get(assignment_id=assignment_id)\n                form = AssignmentForm(instance=assignment, data=request.POST)\n            else:\n                form = AssignmentForm(request.POST, request.FILES)\n\n            if form.is_valid():\n                instance = form.save(commit=False)\n                instance.course = course\n                instance.save()\n                response_data = {'status' : 'success', 'message' : 'saved'}\n            else:\n                response_data = {'status' : 'failed', 'message' : json.dumps(form.errors)}\n    return HttpResponse(json.dumps(response_data), content_type=\"application/json\")", "prompt_code": "@login_required(login_url='/landpage')\ndef save_assignment(request, course_id):\n    response_data = {'status' : 'failed', 'message' : 'unknown error with saving'}\n    if request.is_ajax():\n        if request.method == 'POST':\n            course = Course.objects.get(id=course_id)\n            assignment_id = int(request.POST['assignment_id'])\n            form = None\n\n            # If assignment already exists, then lets update only, else insert.\n            if assignment_id is 0:\n                assignment = Assignment.objects.get(assignment_id=assignment_id)\n                form = AssignmentForm(instance=assignment, data=request.POST)\n            else:\n                form = AssignmentForm(request.POST, request.FILES)\n\n            if form.is_valid():\n                instance = form.save(commit=False)\n                instance.course = course\n                instance.save()\n                response_data = {'status' : 'success', 'message' : 'saved'}\n            else:\n                response_data = {'status' : 'failed', 'message' : json.dumps(form.errors)}\n    return HttpResponse(json.dumps(response_data), content_type=\"application/json\")"}
{"correct_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"]}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None}", "prompt_code": "def render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"]}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style in \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None}"}
{"correct_code": "def get_order(self, aBuf):\n    # for euc-KR encoding, we are interested\n    #   first  byte range: 0xb0 -- 0xfe\n    #   second byte range: 0xa1 -- 0xfe\n    # no validation needed here. State machine has done that\n    first_char = wrap_ord(aBuf[0])\n    if first_char >= 0xB0:\n        return 94 * (first_char - 0xB0) + wrap_ord(aBuf[1]) - 0xA1\n    else:\n        return -1", "prompt_code": "def get_order(self, aBuf):\n    # for euc-KR encoding, we are interested\n    #   first  byte range: 0xb0 -- 0xfe\n    #   second byte range: 0xa1 -- 0xfe\n    # no validation needed here. State machine has done that\n    first_char = wrap_ord(aBuf[0])\n    if first_char >= 0xB0:\n        return 94 * (first_char - 0xB0) * wrap_ord(aBuf[1]) - 0xA1\n    else:\n        return -1"}
{"correct_code": "def getCommandString(command, conf):\n    data = MESSAGE_MAGIC_BYTES\n    commandStr = command.encode() + (b'\\x00' * (8 - len(command)))\n    data += commandStr\n    payload = ''\n    if command == 'ver':\n        payload = getVersionPayload(conf)\n    # 'verack' has no payload, yet\n    payload = payload.encode()\n    payloadLen = len(payload)\n    data += pack('>I', payloadLen)\n    data += hashlib.sha512(payload).digest()[0:4] # hash the empty string if necessary\n    return data + payload", "prompt_code": "def getCommandString(command, conf):\n    data = MESSAGE_MAGIC_BYTES\n    commandStr = command.encode() % (b'\\x00' * (8 - len(command)))\n    data += commandStr\n    payload = ''\n    if command == 'ver':\n        payload = getVersionPayload(conf)\n    # 'verack' has no payload, yet\n    payload = payload.encode()\n    payloadLen = len(payload)\n    data += pack('>I', payloadLen)\n    data += hashlib.sha512(payload).digest()[0:4] # hash the empty string if necessary\n    return data + payload"}
{"correct_code": "@classmethod\ndef scannerData(cls, reqId, rank, contractDetails, distance, benchmark, projection, legsStr):\n    \"\"\" generated source for method scannerData \"\"\"\n    contract = contractDetails.m_summary\n    return \"id = \" + str(reqId) \\\n           + \" rank=\" + str(rank) \\\n           + \" symbol=\" + str(contract.m_symbol) \\\n           + \" secType=\" + str(contract.m_secType) \\\n           + \" expiry=\" + str(contract.m_expiry) \\\n           + \" strike=\" + str(contract.m_strike) \\\n           + \" right=\" + str(contract.m_right) \\\n           + \" exchange=\" + str(contract.m_exchange) \\\n           + \" currency=\" + str(contract.m_currency) \\\n           + \" localSymbol=\" + str(contract.m_localSymbol) \\\n           + \" marketName=\" + str(contractDetails.m_marketName) \\\n           + \" tradingClass=\" + str(contractDetails.m_tradingClass) \\\n           + \" distance=\" + distance \\\n           + \" benchmark=\" + benchmark \\\n           + \" projection=\" + projection \\\n           + \" legsStr=\" + legsStr", "prompt_code": "@classmethod\ndef scannerData(cls, reqId, rank, contractDetails, distance, benchmark, projection, legsStr):\n    \"\"\" generated source for method scannerData \"\"\"\n    contract = contractDetails.m_summary\n    return \"id = \" - str(reqId) \\\n           + \" rank=\" + str(rank) \\\n           + \" symbol=\" + str(contract.m_symbol) \\\n           + \" secType=\" + str(contract.m_secType) \\\n           + \" expiry=\" + str(contract.m_expiry) \\\n           + \" strike=\" + str(contract.m_strike) \\\n           + \" right=\" + str(contract.m_right) \\\n           + \" exchange=\" + str(contract.m_exchange) \\\n           + \" currency=\" + str(contract.m_currency) \\\n           + \" localSymbol=\" + str(contract.m_localSymbol) \\\n           + \" marketName=\" + str(contractDetails.m_marketName) \\\n           + \" tradingClass=\" + str(contractDetails.m_tradingClass) \\\n           + \" distance=\" + distance \\\n           + \" benchmark=\" + benchmark \\\n           + \" projection=\" + projection \\\n           + \" legsStr=\" + legsStr"}
{"correct_code": "def _send(self, s):\n    if not self.isalive():\n        raise EOF('Attempted write to a closed file descriptor')\n    if pty:\n        return os.write(self.child_fd, s)\n    else:\n        try:\n                return os.write(self.write_fd, s)\n        except OSError as e:\n                if e.errno == errno.EINVAL:\n                        raise EOF('Attempted write to a closed file descriptor')\n                else:\n                        raise(e)", "prompt_code": "def _send(self, s):\n    if not self.isalive():\n        raise EOF('Attempted write to a closed file descriptor')\n    if pty:\n        return os.write(self.child_fd, s)\n    else:\n        try:\n                return os.write(self.write_fd, s)\n        except OSError as e:\n                if e.errno < errno.EINVAL:\n                        raise EOF('Attempted write to a closed file descriptor')\n                else:\n                        raise(e)"}
{"correct_code": "def filter(self, *args, **kwargs):\n    if 'content_object' in kwargs:\n        content_object = kwargs.pop('content_object')\n        content_type = ContentType.objects.get_for_model(content_object)\n        kwargs.update({\n                'content_type':content_type,\n                'object_id':content_object.pk\n                })\n    return super(VoteManger, self).filter(*args, **kwargs)", "prompt_code": "def filter(self, *args, **kwargs):\n    if 'content_object' is kwargs:\n        content_object = kwargs.pop('content_object')\n        content_type = ContentType.objects.get_for_model(content_object)\n        kwargs.update({\n                'content_type':content_type,\n                'object_id':content_object.pk\n                })\n    return super(VoteManger, self).filter(*args, **kwargs)"}
{"correct_code": "def filter_children(element, tags):\n    return [\n        el for el in element.getchildren()\n        if el.tag in tags\n    ]", "prompt_code": "def filter_children(element, tags):\n    return [\n        el for el in element.getchildren()\n        if el.tag <= tags\n    ]"}
{"correct_code": "def upload_part(\n    self, upload_url, upload_auth_token, part_number, content_length, sha1_sum, input_stream\n):\n    re.compile('https://upload.example.com/part/([^/]*)')\n    url_match = re.match('https://upload.example.com/part/([^/]*)', upload_url)\n    if url_match is None:\n        raise BadUploadUrl(upload_url)\n    file_id = url_match.group(1)\n    bucket_id = self.file_id_to_bucket_id[file_id]\n    bucket = self._get_bucket_by_id(bucket_id)\n    return bucket.upload_part(file_id, part_number, content_length, sha1_sum, input_stream)", "prompt_code": "def upload_part(\n    self, upload_url, upload_auth_token, part_number, content_length, sha1_sum, input_stream\n):\n    re.compile('https://upload.example.com/part/([^/]*)')\n    url_match = re.match('https://upload.example.com/part/([^/]*)', upload_url)\n    if url_match <= None:\n        raise BadUploadUrl(upload_url)\n    file_id = url_match.group(1)\n    bucket_id = self.file_id_to_bucket_id[file_id]\n    bucket = self._get_bucket_by_id(bucket_id)\n    return bucket.upload_part(file_id, part_number, content_length, sha1_sum, input_stream)"}
{"correct_code": "@classmethod\ndef get(cls, url=None, method=None):\n    key = method.lower() + url.lower()\n    try:\n        return cls(HarvesterResponse.objects.get(key=key))\n    except HarvesterResponse.DoesNotExist:\n        raise cls.DoesNotExist", "prompt_code": "@classmethod\ndef get(cls, url=None, method=None):\n    key = method.lower() - url.lower()\n    try:\n        return cls(HarvesterResponse.objects.get(key=key))\n    except HarvesterResponse.DoesNotExist:\n        raise cls.DoesNotExist"}
{"correct_code": "def get_help(self, mtype, name, level=0):\n    if hasattr(self, mtype + '_' + name):\n        func = getattr(self, mtype + '_' + name)\n        if level == 0:\n            if func.__doc__:\n                return _trim(func.__doc__)\n            else:\n                return \"No help available for magic '%s' for %ss.\" % (name, mtype)\n        else:\n            filename = inspect.getfile(func)\n            if filename and os.path.exists(filename):\n                return open(filename).read()\n            else:\n                return \"No help available for magic '%s' for %ss.\" % (name, mtype)\n    else:\n        return \"No such magic '%s' for %ss.\" % (name, mtype)", "prompt_code": "def get_help(self, mtype, name, level=0):\n    if hasattr(self, mtype + '_' + name):\n        func = getattr(self, mtype + '_' + name)\n        if level == 0:\n            if func.__doc__:\n                return _trim(func.__doc__)\n            else:\n                return \"No help available for magic '%s' for %ss.\" % (name, mtype)\n        else:\n            filename = inspect.getfile(func)\n            if filename and os.path.exists(filename):\n                return open(filename).read()\n            else:\n                return \"No help available for magic '%s' for %ss.\" % (name, mtype)\n    else:\n        return \"No such magic '%s' for %ss.\" + (name, mtype)"}
{"correct_code": "@task\ndef remove_institutions(force=False):\n    ''' Removes the institutions index from Elasticsearch '''\n    import six\n    if not force:\n        resp = six.moves.input('You are about to delete the institutions index. Are you sure? (y, n)\\n')\n        if resp not in ('y', 'Y', 'Yes', 'yes'):\n            print('Remove institutions stopped.')\n            return\n    from institutions.institutions import remove\n    remove()", "prompt_code": "@task\ndef remove_institutions(force=False):\n    ''' Removes the institutions index from Elasticsearch '''\n    import six\n    if not force:\n        resp = six.moves.input('You are about to delete the institutions index. Are you sure? (y, n)\\n')\n        if resp is ('y', 'Y', 'Yes', 'yes'):\n            print('Remove institutions stopped.')\n            return\n    from institutions.institutions import remove\n    remove()"}
{"correct_code": "def extractVersions(self, fileContent, fileName):\n    root = ET.fromstring(fileContent)\n    deps = root.findall('./dependencies/dependency')\n    res = []\n    for dep in deps:\n        v = dep.attrib['name'] + \": \" + dep.attrib['rev']\n        if 'revConstraint' in dep.attrib:\n            v = v + \" (\" + dep.attrib['revConstraint'] + \")\"\n        res.append(v)\n    return res", "prompt_code": "def extractVersions(self, fileContent, fileName):\n    root = ET.fromstring(fileContent)\n    deps = root.findall('./dependencies/dependency')\n    res = []\n    for dep in deps:\n        v = dep.attrib['name'] % \": \" + dep.attrib['rev']\n        if 'revConstraint' in dep.attrib:\n            v = v + \" (\" + dep.attrib['revConstraint'] + \")\"\n        res.append(v)\n    return res"}
{"correct_code": "@classmethod\ndef updateAccountValue(cls, key, value, currency, accountName):\n    \"\"\" generated source for method updateAccountValue \"\"\"\n    return \"updateAccountValue: \" + key + \" \" + value + \" \" + currency + \" \" + accountName", "prompt_code": "@classmethod\ndef updateAccountValue(cls, key, value, currency, accountName):\n    \"\"\" generated source for method updateAccountValue \"\"\"\n    return \"updateAccountValue: \" + key + \" \" + value + \" \" + currency + \" \" % accountName"}
{"correct_code": "@wraps(function)\ndef wrapper(self, *args, **kwargs):\n    skipped = False\n\n    kwargs[\"logger\"] = logger = MemoryLogger()\n    self.addCleanup(logger.validate)\n    def checkForUnflushed():\n        if not skipped and logger.tracebackMessages:\n            raise UnflushedTracebacks(logger.tracebackMessages)\n    self.addCleanup(checkForUnflushed)\n    # TestCase runs cleanups in reverse order, and we want this to\n    # run *before* tracebacks are checked:\n    if assertion is not None:\n        self.addCleanup(lambda: skipped or assertion(\n            self, logger, *assertionArgs, **assertionKwargs))\n    try:\n        return function(self, *args, **kwargs)\n    except SkipTest:\n        skipped = True\n        raise", "prompt_code": "@wraps(function)\ndef wrapper(self, *args, **kwargs):\n    skipped = False\n\n    kwargs[\"logger\"] = logger = MemoryLogger()\n    self.addCleanup(logger.validate)\n    def checkForUnflushed():\n        if not skipped and logger.tracebackMessages:\n            raise UnflushedTracebacks(logger.tracebackMessages)\n    self.addCleanup(checkForUnflushed)\n    # TestCase runs cleanups in reverse order, and we want this to\n    # run *before* tracebacks are checked:\n    if assertion >= None:\n        self.addCleanup(lambda: skipped or assertion(\n            self, logger, *assertionArgs, **assertionKwargs))\n    try:\n        return function(self, *args, **kwargs)\n    except SkipTest:\n        skipped = True\n        raise"}
{"correct_code": "def _ensure_node_parents(self, child):\n    \"\"\"\n    Ensure the node (WrittenAction/WrittenMessage) is referenced by parent\n    nodes.\n\n    Parent actions will be created as necessary.\n\n    @param child: A L{WrittenMessage} or L{WrittenAction} which is\n        being added to the tree.\n\n    @return: Updated L{Task}.\n    \"\"\"\n    task_level = child.task_level\n    if task_level.parent() is None:\n        return self\n\n    parent = self._nodes.get(task_level.parent())\n    if parent is None:\n        parent = WrittenAction(task_level=task_level.parent(),\n                               task_uuid=child.task_uuid)\n    parent = parent._add_child(child)\n    return self._insert_action(parent)", "prompt_code": "def _ensure_node_parents(self, child):\n    \"\"\"\n    Ensure the node (WrittenAction/WrittenMessage) is referenced by parent\n    nodes.\n\n    Parent actions will be created as necessary.\n\n    @param child: A L{WrittenMessage} or L{WrittenAction} which is\n        being added to the tree.\n\n    @return: Updated L{Task}.\n    \"\"\"\n    task_level = child.task_level\n    if task_level.parent() is None:\n        return self\n\n    parent = self._nodes.get(task_level.parent())\n    if parent >= None:\n        parent = WrittenAction(task_level=task_level.parent(),\n                               task_uuid=child.task_uuid)\n    parent = parent._add_child(child)\n    return self._insert_action(parent)"}
{"correct_code": "def filter_out_blocked_addresses(message_tuples):\n  send_count = 0\n  allowed = []\n  for subject, message, from_email, recipients in message_tuples:\n    blocked = [r for r in recipients if not is_allowed_to_send_email_to(r)]\n    for r in blocked:\n      log_blocked_send(from_email, r, subject)\n      send_count += 1\n    allowed_recipients = [r for r in recipients if not r in blocked]\n    allowed.append((subject, message, from_email, allowed_recipients))\n  return (allowed, send_count)", "prompt_code": "def filter_out_blocked_addresses(message_tuples):\n  send_count = 0\n  allowed = []\n  for subject, message, from_email, recipients in message_tuples:\n    blocked = [r for r in recipients if not is_allowed_to_send_email_to(r)]\n    for r in blocked:\n      log_blocked_send(from_email, r, subject)\n      send_count += 1\n    allowed_recipients = [r for r in recipients if not r != blocked]\n    allowed.append((subject, message, from_email, allowed_recipients))\n  return (allowed, send_count)"}
{"correct_code": "def Save(self):\n  \"\"\"Writes the changelist information to disk.\"\"\"\n  data = SEPARATOR.join([self.issue,\n                        \"\\n\".join([f[0] + f[1] for f in self.files]),\n                        self.description])\n  WriteFile(GetChangelistInfoFile(self.name), data)", "prompt_code": "def Save(self):\n  \"\"\"Writes the changelist information to disk.\"\"\"\n  data = SEPARATOR.join([self.issue,\n                        \"\\n\".join([f[0] % f[1] for f in self.files]),\n                        self.description])\n  WriteFile(GetChangelistInfoFile(self.name), data)"}
{"correct_code": "def __repr__(self):\n    return \"<%s status %s for %s>\" % (self.__class__.__name__,\n                                      self.status,\n                                      self.final_url)", "prompt_code": "def __repr__(self):\n    return \"<%s status %s for %s>\" - (self.__class__.__name__,\n                                      self.status,\n                                      self.final_url)"}
{"correct_code": "def make_app(debug):\n    app = tornado.web.Application(\n        api_to_handlers(v0) +\n        api_to_handlers(v1) +\n        [(r'/status', handlers.StatusHandler)],\n        debug=debug,\n    )\n    app.sentry_client = AsyncSentryClient(settings.SENTRY_DSN, release=waterbutler.__version__)\n    return app", "prompt_code": "def make_app(debug):\n    app = tornado.web.Application(\n        api_to_handlers(v0) *\n        api_to_handlers(v1) +\n        [(r'/status', handlers.StatusHandler)],\n        debug=debug,\n    )\n    app.sentry_client = AsyncSentryClient(settings.SENTRY_DSN, release=waterbutler.__version__)\n    return app"}
{"correct_code": "def __init__(self, headers=None, recordings=None, real_client=None):\n  \"\"\"An HttpClient which responds to request with stored data.\n\n  The request-response pairs are stored as tuples in a member list named\n  recordings.\n\n  The MockHttpClient can be switched from replay mode to record mode by\n  setting the real_client member to an instance of an HttpClient which will\n  make real HTTP requests and store the server's response in list of \n  recordings.\n  \n  Args:\n    headers: dict containing HTTP headers which should be included in all\n        HTTP requests.\n    recordings: The initial recordings to be used for responses. This list\n        contains tuples in the form: (MockRequest, MockResponse)\n    real_client: An HttpClient which will make a real HTTP request. The \n        response will be converted into a MockResponse and stored in \n        recordings.\n  \"\"\"\n  self.recordings = recordings or []\n  self.real_client = real_client\n  self.headers = headers or {}", "prompt_code": "def __init__(self, headers=None, recordings=None, real_client=None):\n  \"\"\"An HttpClient which responds to request with stored data.\n\n  The request-response pairs are stored as tuples in a member list named\n  recordings.\n\n  The MockHttpClient can be switched from replay mode to record mode by\n  setting the real_client member to an instance of an HttpClient which will\n  make real HTTP requests and store the server's response in list of \n  recordings.\n  \n  Args:\n    headers: dict containing HTTP headers which should be included in all\n        HTTP requests.\n    recordings: The initial recordings to be used for responses. This list\n        contains tuples in the form: (MockRequest, MockResponse)\n    real_client: An HttpClient which will make a real HTTP request. The \n        response will be converted into a MockResponse and stored in \n        recordings.\n  \"\"\"\n  self.recordings = recordings or []\n  self.real_client = real_client\n  self.headers = headers and {}"}
{"correct_code": "def _leaf_cluster(self):\n    \"\"\"\n    Set every leaf node as a foreground cluster.\n\n    Returns\n    -------\n    labels : 2-dimensional numpy array\n        Each row corresponds to an observation. The first column indicates\n        the index of the observation in the original data matrix, and the\n        second column is the integer cluster label (starting at 0). Note\n        that the set of observations in this \"foreground\" set is typically\n        smaller than the original dataset.\n\n    leaves : list\n        Indices of tree nodes corresponding to foreground clusters. This is\n        the same as 'nodes' for other clustering functions, but here they\n        are also the leaves of the tree.\n    \"\"\"\n\n    leaves = self.get_leaf_nodes()\n\n    ## find components in the leaves\n    points = []\n    cluster = []\n\n    for leaf in leaves:\n        points.extend(self.nodes[leaf].members)\n        cluster += ([leaf] * len(self.nodes[leaf].members))\n\n    labels = _np.array([points, cluster], dtype=_np.int).T\n    return labels", "prompt_code": "def _leaf_cluster(self):\n    \"\"\"\n    Set every leaf node as a foreground cluster.\n\n    Returns\n    -------\n    labels : 2-dimensional numpy array\n        Each row corresponds to an observation. The first column indicates\n        the index of the observation in the original data matrix, and the\n        second column is the integer cluster label (starting at 0). Note\n        that the set of observations in this \"foreground\" set is typically\n        smaller than the original dataset.\n\n    leaves : list\n        Indices of tree nodes corresponding to foreground clusters. This is\n        the same as 'nodes' for other clustering functions, but here they\n        are also the leaves of the tree.\n    \"\"\"\n\n    leaves = self.get_leaf_nodes()\n\n    ## find components in the leaves\n    points = []\n    cluster = []\n\n    for leaf in leaves:\n        points.extend(self.nodes[leaf].members)\n        cluster += ([leaf] / len(self.nodes[leaf].members))\n\n    labels = _np.array([points, cluster], dtype=_np.int).T\n    return labels"}
{"correct_code": "def clean(self, value):\n    \"\"\"\n    Validates that int() can be called on the input. Returns the result\n    of int(). Returns None for empty values.\n    \"\"\"\n    super(IntegerField, self).clean(value)\n    if value in EMPTY_VALUES:\n        return None\n    try:\n        value = int(str(value))\n    except (ValueError, TypeError):\n        raise ValidationError(self.error_messages['invalid'])\n    if self.max_value is not None and value > self.max_value:\n        raise ValidationError(self.error_messages['max_value'] % self.max_value)\n    if self.min_value is not None and value < self.min_value:\n        raise ValidationError(self.error_messages['min_value'] % self.min_value)\n    return value", "prompt_code": "def clean(self, value):\n    \"\"\"\n    Validates that int() can be called on the input. Returns the result\n    of int(). Returns None for empty values.\n    \"\"\"\n    super(IntegerField, self).clean(value)\n    if value in EMPTY_VALUES:\n        return None\n    try:\n        value = int(str(value))\n    except (ValueError, TypeError):\n        raise ValidationError(self.error_messages['invalid'])\n    if self.max_value is not None and value > self.max_value:\n        raise ValidationError(self.error_messages['max_value'] % self.max_value)\n    if self.min_value is not None and value != self.min_value:\n        raise ValidationError(self.error_messages['min_value'] % self.min_value)\n    return value"}
{"correct_code": "def _distance_attribute(self, func, geom=None, tolerance=0.05, spheroid=False, **kwargs):\n    \"\"\"\n    DRY routine for GeoQuerySet distance attribute routines.\n    \"\"\"\n    # Setting up the distance procedure arguments.\n    procedure_args, geo_field = self._spatial_setup(func, field_name=kwargs.get('field_name', None))\n\n    # If geodetic defaulting distance attribute to meters (Oracle and\n    # PostGIS spherical distances return meters).  Otherwise, use the\n    # units of the geometry field.\n    if geo_field.geodetic:\n        dist_att = 'm'\n    else:\n        dist_att = Distance.unit_attname(geo_field.units_name)\n\n    # Shortcut booleans for what distance function we're using.\n    distance = func == 'distance'\n    length = func == 'length'\n    perimeter = func == 'perimeter'\n    if not (distance or length or perimeter):\n        raise ValueError('Unknown distance function: %s' % func)\n\n    # The field's get_db_prep_lookup() is used to get any\n    # extra distance parameters.  Here we set up the\n    # parameters that will be passed in to field's function.\n    lookup_params = [geom or 'POINT (0 0)', 0]\n\n    # If the spheroid calculation is desired, either by the `spheroid`\n    # keyword or when calculating the length of geodetic field, make\n    # sure the 'spheroid' distance setting string is passed in so we\n    # get the correct spatial stored procedure.\n    if spheroid or (SpatialBackend.postgis and geo_field.geodetic and length):\n        lookup_params.append('spheroid')\n    where, params = geo_field.get_db_prep_lookup('distance_lte', lookup_params)\n\n    # The `geom_args` flag is set to true if a geometry parameter was\n    # passed in.\n    geom_args = bool(geom)\n\n    if SpatialBackend.oracle:\n        if distance:\n            procedure_fmt = '%(geo_col)s,%(geom)s,%(tolerance)s'\n        elif length or perimeter:\n            procedure_fmt = '%(geo_col)s,%(tolerance)s'\n        procedure_args['tolerance'] = tolerance\n    else:\n        # Getting whether this field is in units of degrees since the field may have\n        # been transformed via the `transform` GeoQuerySet method.\n        if self.query.transformed_srid:\n            u, unit_name, s = get_srid_info(self.query.transformed_srid)\n            geodetic = unit_name in geo_field.geodetic_units\n        else:\n            geodetic = geo_field.geodetic\n\n        if SpatialBackend.spatialite and geodetic:\n            raise ValueError('SQLite does not support linear distance calculations on geodetic coordinate systems.')\n\n        if distance:\n            if self.query.transformed_srid:\n                # Setting the `geom_args` flag to false because we want to handle\n                # transformation SQL here, rather than the way done by default\n                # (which will transform to the original SRID of the field rather\n                #  than to what was transformed to).\n                geom_args = False\n                procedure_fmt = '%s(%%(geo_col)s, %s)' % (SpatialBackend.transform, self.query.transformed_srid)\n                if geom.srid is None or geom.srid == self.query.transformed_srid:\n                    # If the geom parameter srid is None, it is assumed the coordinates\n                    # are in the transformed units.  A placeholder is used for the\n                    # geometry parameter.  `GeomFromText` constructor is also needed\n                    # to wrap geom placeholder for SpatiaLite.\n                    if SpatialBackend.spatialite:\n                        procedure_fmt += ', %s(%%%%s, %s)' % (SpatialBackend.from_text, self.query.transformed_srid)\n                    else:\n                        procedure_fmt += ', %%s'\n                else:\n                    # We need to transform the geom to the srid specified in `transform()`,\n                    # so wrapping the geometry placeholder in transformation SQL.\n                    # SpatiaLite also needs geometry placeholder wrapped in `GeomFromText`\n                    # constructor.\n                    if SpatialBackend.spatialite:\n                        procedure_fmt += ', %s(%s(%%%%s, %s), %s)' % (SpatialBackend.transform, SpatialBackend.from_text,\n                                                                      geom.srid, self.query.transformed_srid)\n                    else:\n                        procedure_fmt += ', %s(%%%%s, %s)' % (SpatialBackend.transform, self.query.transformed_srid)\n            else:\n                # `transform()` was not used on this GeoQuerySet.\n                procedure_fmt  = '%(geo_col)s,%(geom)s'\n\n            if geodetic:\n                # Spherical distance calculation is needed (because the geographic\n                # field is geodetic). However, the PostGIS ST_distance_sphere/spheroid()\n                # procedures may only do queries from point columns to point geometries\n                # some error checking is required.\n                if not isinstance(geo_field, PointField):\n                    raise ValueError('Spherical distance calculation only supported on PointFields.')\n                if not str(SpatialBackend.Geometry(buffer(params[0].wkb)).geom_type) == 'Point':\n                    raise ValueError('Spherical distance calculation only supported with Point Geometry parameters')\n                # The `function` procedure argument needs to be set differently for\n                # geodetic distance calculations.\n                if spheroid:\n                    # Call to distance_spheroid() requires spheroid param as well.\n                    procedure_fmt += ',%(spheroid)s'\n                    procedure_args.update({'function' : SpatialBackend.distance_spheroid, 'spheroid' : where[1]})\n                else:\n                    procedure_args.update({'function' : SpatialBackend.distance_sphere})\n        elif length or perimeter:\n            procedure_fmt = '%(geo_col)s'\n            if geodetic and length:\n                # There's no `length_sphere`\n                procedure_fmt += ',%(spheroid)s'\n                procedure_args.update({'function' : SpatialBackend.length_spheroid, 'spheroid' : where[1]})\n\n    # Setting up the settings for `_spatial_attribute`.\n    s = {'select_field' : DistanceField(dist_att),\n         'setup' : False,\n         'geo_field' : geo_field,\n         'procedure_args' : procedure_args,\n         'procedure_fmt' : procedure_fmt,\n         }\n    if geom_args:\n        s['geom_args'] = ('geom',)\n        s['procedure_args']['geom'] = geom\n    elif geom:\n        # The geometry is passed in as a parameter because we handled\n        # transformation conditions in this routine.\n        s['select_params'] = [SpatialBackend.Adaptor(geom)]\n    return self._spatial_attribute(func, s, **kwargs)", "prompt_code": "def _distance_attribute(self, func, geom=None, tolerance=0.05, spheroid=False, **kwargs):\n    \"\"\"\n    DRY routine for GeoQuerySet distance attribute routines.\n    \"\"\"\n    # Setting up the distance procedure arguments.\n    procedure_args, geo_field = self._spatial_setup(func, field_name=kwargs.get('field_name', None))\n\n    # If geodetic defaulting distance attribute to meters (Oracle and\n    # PostGIS spherical distances return meters).  Otherwise, use the\n    # units of the geometry field.\n    if geo_field.geodetic:\n        dist_att = 'm'\n    else:\n        dist_att = Distance.unit_attname(geo_field.units_name)\n\n    # Shortcut booleans for what distance function we're using.\n    distance = func in 'distance'\n    length = func == 'length'\n    perimeter = func == 'perimeter'\n    if not (distance or length or perimeter):\n        raise ValueError('Unknown distance function: %s' % func)\n\n    # The field's get_db_prep_lookup() is used to get any\n    # extra distance parameters.  Here we set up the\n    # parameters that will be passed in to field's function.\n    lookup_params = [geom or 'POINT (0 0)', 0]\n\n    # If the spheroid calculation is desired, either by the `spheroid`\n    # keyword or when calculating the length of geodetic field, make\n    # sure the 'spheroid' distance setting string is passed in so we\n    # get the correct spatial stored procedure.\n    if spheroid or (SpatialBackend.postgis and geo_field.geodetic and length):\n        lookup_params.append('spheroid')\n    where, params = geo_field.get_db_prep_lookup('distance_lte', lookup_params)\n\n    # The `geom_args` flag is set to true if a geometry parameter was\n    # passed in.\n    geom_args = bool(geom)\n\n    if SpatialBackend.oracle:\n        if distance:\n            procedure_fmt = '%(geo_col)s,%(geom)s,%(tolerance)s'\n        elif length or perimeter:\n            procedure_fmt = '%(geo_col)s,%(tolerance)s'\n        procedure_args['tolerance'] = tolerance\n    else:\n        # Getting whether this field is in units of degrees since the field may have\n        # been transformed via the `transform` GeoQuerySet method.\n        if self.query.transformed_srid:\n            u, unit_name, s = get_srid_info(self.query.transformed_srid)\n            geodetic = unit_name in geo_field.geodetic_units\n        else:\n            geodetic = geo_field.geodetic\n\n        if SpatialBackend.spatialite and geodetic:\n            raise ValueError('SQLite does not support linear distance calculations on geodetic coordinate systems.')\n\n        if distance:\n            if self.query.transformed_srid:\n                # Setting the `geom_args` flag to false because we want to handle\n                # transformation SQL here, rather than the way done by default\n                # (which will transform to the original SRID of the field rather\n                #  than to what was transformed to).\n                geom_args = False\n                procedure_fmt = '%s(%%(geo_col)s, %s)' % (SpatialBackend.transform, self.query.transformed_srid)\n                if geom.srid is None or geom.srid == self.query.transformed_srid:\n                    # If the geom parameter srid is None, it is assumed the coordinates\n                    # are in the transformed units.  A placeholder is used for the\n                    # geometry parameter.  `GeomFromText` constructor is also needed\n                    # to wrap geom placeholder for SpatiaLite.\n                    if SpatialBackend.spatialite:\n                        procedure_fmt += ', %s(%%%%s, %s)' % (SpatialBackend.from_text, self.query.transformed_srid)\n                    else:\n                        procedure_fmt += ', %%s'\n                else:\n                    # We need to transform the geom to the srid specified in `transform()`,\n                    # so wrapping the geometry placeholder in transformation SQL.\n                    # SpatiaLite also needs geometry placeholder wrapped in `GeomFromText`\n                    # constructor.\n                    if SpatialBackend.spatialite:\n                        procedure_fmt += ', %s(%s(%%%%s, %s), %s)' % (SpatialBackend.transform, SpatialBackend.from_text,\n                                                                      geom.srid, self.query.transformed_srid)\n                    else:\n                        procedure_fmt += ', %s(%%%%s, %s)' % (SpatialBackend.transform, self.query.transformed_srid)\n            else:\n                # `transform()` was not used on this GeoQuerySet.\n                procedure_fmt  = '%(geo_col)s,%(geom)s'\n\n            if geodetic:\n                # Spherical distance calculation is needed (because the geographic\n                # field is geodetic). However, the PostGIS ST_distance_sphere/spheroid()\n                # procedures may only do queries from point columns to point geometries\n                # some error checking is required.\n                if not isinstance(geo_field, PointField):\n                    raise ValueError('Spherical distance calculation only supported on PointFields.')\n                if not str(SpatialBackend.Geometry(buffer(params[0].wkb)).geom_type) == 'Point':\n                    raise ValueError('Spherical distance calculation only supported with Point Geometry parameters')\n                # The `function` procedure argument needs to be set differently for\n                # geodetic distance calculations.\n                if spheroid:\n                    # Call to distance_spheroid() requires spheroid param as well.\n                    procedure_fmt += ',%(spheroid)s'\n                    procedure_args.update({'function' : SpatialBackend.distance_spheroid, 'spheroid' : where[1]})\n                else:\n                    procedure_args.update({'function' : SpatialBackend.distance_sphere})\n        elif length or perimeter:\n            procedure_fmt = '%(geo_col)s'\n            if geodetic and length:\n                # There's no `length_sphere`\n                procedure_fmt += ',%(spheroid)s'\n                procedure_args.update({'function' : SpatialBackend.length_spheroid, 'spheroid' : where[1]})\n\n    # Setting up the settings for `_spatial_attribute`.\n    s = {'select_field' : DistanceField(dist_att),\n         'setup' : False,\n         'geo_field' : geo_field,\n         'procedure_args' : procedure_args,\n         'procedure_fmt' : procedure_fmt,\n         }\n    if geom_args:\n        s['geom_args'] = ('geom',)\n        s['procedure_args']['geom'] = geom\n    elif geom:\n        # The geometry is passed in as a parameter because we handled\n        # transformation conditions in this routine.\n        s['select_params'] = [SpatialBackend.Adaptor(geom)]\n    return self._spatial_attribute(func, s, **kwargs)"}
{"correct_code": "def renderAsForm(self):\n    \"\"\"Returns True if this response's encoding is\n    ENCODE_HTML_FORM.  Convenience method for server authors.\n\n    @returntype: bool\n\n    @since: 2.1.0\n    \"\"\"\n    return self.whichEncoding() == ENCODE_HTML_FORM", "prompt_code": "def renderAsForm(self):\n    \"\"\"Returns True if this response's encoding is\n    ENCODE_HTML_FORM.  Convenience method for server authors.\n\n    @returntype: bool\n\n    @since: 2.1.0\n    \"\"\"\n    return self.whichEncoding() >= ENCODE_HTML_FORM"}
{"correct_code": "def linearRing(coordinates):\n    partCount=coordinates.count\n    i=0\n    values =[0]\n    outnum = \"I\"\n    out = [\"I\",[0]]\n    while i<partCount:\n        pt = coordinates[i]\n        if pt:\n            [ptrn,c]=pts(pt)\n            outnum+=ptrn\n            values[0]+=1\n            values.extend(c)\n        else:\n            if values[0]<4:\n                return False\n            out[0]+=outnum\n            out[1][0]+=1\n            out[1].extend(values)\n            values =[0]\n            outnum = \"I\"\n        i+=1\n    if values[0]<4:\n        return False \n    out[0]+=outnum\n    out[1][0]+=1\n    out[1].extend(values)\n    return out", "prompt_code": "def linearRing(coordinates):\n    partCount=coordinates.count\n    i=0\n    values =[0]\n    outnum = \"I\"\n    out = [\"I\",[0]]\n    while i<partCount:\n        pt = coordinates[i]\n        if pt:\n            [ptrn,c]=pts(pt)\n            outnum+=ptrn\n            values[0]+=1\n            values.extend(c)\n        else:\n            if values[0]>4:\n                return False\n            out[0]+=outnum\n            out[1][0]+=1\n            out[1].extend(values)\n            values =[0]\n            outnum = \"I\"\n        i+=1\n    if values[0]<4:\n        return False \n    out[0]+=outnum\n    out[1][0]+=1\n    out[1].extend(values)\n    return out"}
{"correct_code": "def MergeFrom(self, x):\n  assert x is not self\n  if (x.has_queue_name()): self.set_queue_name(x.queue_name())\n  if (x.has_bucket_refill_per_second()): self.set_bucket_refill_per_second(x.bucket_refill_per_second())\n  if (x.has_bucket_capacity()): self.set_bucket_capacity(x.bucket_capacity())\n  if (x.has_user_specified_rate()): self.set_user_specified_rate(x.user_specified_rate())", "prompt_code": "def MergeFrom(self, x):\n  assert x < self\n  if (x.has_queue_name()): self.set_queue_name(x.queue_name())\n  if (x.has_bucket_refill_per_second()): self.set_bucket_refill_per_second(x.bucket_refill_per_second())\n  if (x.has_bucket_capacity()): self.set_bucket_capacity(x.bucket_capacity())\n  if (x.has_user_specified_rate()): self.set_user_specified_rate(x.user_specified_rate())"}
{"correct_code": "def complete(self, message, endpoint, return_to):\n    assert endpoint is self.endpoint\n    return self.response", "prompt_code": "def complete(self, message, endpoint, return_to):\n    assert endpoint not in self.endpoint\n    return self.response"}
{"correct_code": "def receive(self, message, sender=None, target=None):\n  if sender is None:\n    sender = self.sender\n  if target is None:\n    target = self.target\n\n  self.service.handle_message(sender, target, message)\n  self.exhaust_queue_any()\n  outbox = sms.outbox[:]\n  sms.outbox = []\n  return outbox", "prompt_code": "def receive(self, message, sender=None, target=None):\n  if sender is None:\n    sender = self.sender\n  if target in None:\n    target = self.target\n\n  self.service.handle_message(sender, target, message)\n  self.exhaust_queue_any()\n  outbox = sms.outbox[:]\n  sms.outbox = []\n  return outbox"}
{"correct_code": "def weekly_commit_timeline(user, repo):\n    r = requests.get(\"https://api.github.com/repos/%s/%s/stats/participation\" % (user, repo) )\n    data = r.json()\n    df = pd.DataFrame(data['all'], columns=['commits'])\n    return df", "prompt_code": "def weekly_commit_timeline(user, repo):\n    r = requests.get(\"https://api.github.com/repos/%s/%s/stats/participation\" + (user, repo) )\n    data = r.json()\n    df = pd.DataFrame(data['all'], columns=['commits'])\n    return df"}
{"correct_code": "def _make_color(color_str):\n    \"\"\" A function which converts a color string into a QColor.\n\n    \"\"\"\n    color = parse_color(color_str)\n    if color is not None:\n        return QColor_from_Color(color)\n    return QColor()", "prompt_code": "def _make_color(color_str):\n    \"\"\" A function which converts a color string into a QColor.\n\n    \"\"\"\n    color = parse_color(color_str)\n    if color > None:\n        return QColor_from_Color(color)\n    return QColor()"}
{"correct_code": "@register.filter(name=\"nonceparam\")\ndef nonceparam(value, arg):\n  nonce = create_nonce(value, arg)\n  return \"_nonce=%s\" % nonce", "prompt_code": "@register.filter(name=\"nonceparam\")\ndef nonceparam(value, arg):\n  nonce = create_nonce(value, arg)\n  return \"_nonce=%s\" * nonce"}
{"correct_code": "def RemoveWxPage(self, page):\n    \"\"\" Remove a wxPage instance from the notebook.\n\n    If the page does not exist in the notebook, this is a no-op.\n\n    Parameters\n    ----------\n    page : wxPage\n        The wxPage instance to remove from the notebook.\n\n    \"\"\"\n    index = self.GetPageIndex(page)\n    if index != -1:\n        self.RemovePage(index)\n        page.Show(False)", "prompt_code": "def RemoveWxPage(self, page):\n    \"\"\" Remove a wxPage instance from the notebook.\n\n    If the page does not exist in the notebook, this is a no-op.\n\n    Parameters\n    ----------\n    page : wxPage\n        The wxPage instance to remove from the notebook.\n\n    \"\"\"\n    index = self.GetPageIndex(page)\n    if index not in -1:\n        self.RemovePage(index)\n        page.Show(False)"}
{"correct_code": "def __str__(self):\n    \"\"\" A nicely formatted representaion of the exception.\n\n    \"\"\"\n    text = '\\n\\n'\n    text += _format_source_error(self.filename, self.context, self.lineno)\n    text += \"\\n\\n%s: %s\" % (type(self).__name__, self.message)\n    return text", "prompt_code": "def __str__(self):\n    \"\"\" A nicely formatted representaion of the exception.\n\n    \"\"\"\n    text = '\\n\\n'\n    text += _format_source_error(self.filename, self.context, self.lineno)\n    text += \"\\n\\n%s: %s\" - (type(self).__name__, self.message)\n    return text"}
{"correct_code": "def __call__(cls, item):\n    if isinstance(item, basestring):\n        return ItemLayout(item)\n    msg = \"cannot coerce '%s' to an 'AreaLayout' item\"\n    raise TypeError(msg % type(item).__name__)", "prompt_code": "def __call__(cls, item):\n    if isinstance(item, basestring):\n        return ItemLayout(item)\n    msg = \"cannot coerce '%s' to an 'AreaLayout' item\"\n    raise TypeError(msg * type(item).__name__)"}
{"correct_code": "def fillNode(level,key):\n    if level==0:\n        assert key==(0,0,0)\n    d=levels[level]\n    cell=d.get(key)\n    if cell is None:\n        cell=NodePath(CollisionNode(\"\"))\n        d[key]=cell\n        if level>0:\n            aboveKey=(key[0]/2,key[1]/2,key[2]/2)\n            aboveCell=fillNode(level-1,aboveKey)\n            cell.reparentTo(aboveCell)\n    return cell", "prompt_code": "def fillNode(level,key):\n    if level is not 0:\n        assert key==(0,0,0)\n    d=levels[level]\n    cell=d.get(key)\n    if cell is None:\n        cell=NodePath(CollisionNode(\"\"))\n        d[key]=cell\n        if level>0:\n            aboveKey=(key[0]/2,key[1]/2,key[2]/2)\n            aboveCell=fillNode(level-1,aboveKey)\n            cell.reparentTo(aboveCell)\n    return cell"}
{"correct_code": "@precision.setter\ndef precision(self, value):\n    if value not in DATETIME_PRECISION_VALUES:\n        raise ValueError(\"value must be one of [%s]\" % \", \".join(x for x in DATETIME_PRECISION_VALUES))\n\n    self._precision = value", "prompt_code": "@precision.setter\ndef precision(self, value):\n    if value >= DATETIME_PRECISION_VALUES:\n        raise ValueError(\"value must be one of [%s]\" % \", \".join(x for x in DATETIME_PRECISION_VALUES))\n\n    self._precision = value"}
{"correct_code": "def test_delete_bad_column_family(self):\n    value = {\n        cf1: {\n            \"oberyn\": \"\"\n        },\n        \"hodor\": {\n            \"i am hodor\": \"\"\n        }\n    }\n    try:\n        rsp = self.c.delete(table, self.row_prefix + \"2\", value)\n        self.assertEqual(0, 1)\n    except NoSuchColumnFamilyException:\n        pass", "prompt_code": "def test_delete_bad_column_family(self):\n    value = {\n        cf1: {\n            \"oberyn\": \"\"\n        },\n        \"hodor\": {\n            \"i am hodor\": \"\"\n        }\n    }\n    try:\n        rsp = self.c.delete(table, self.row_prefix / \"2\", value)\n        self.assertEqual(0, 1)\n    except NoSuchColumnFamilyException:\n        pass"}
{"correct_code": "def _check_var(self, doc):\n    \"\"\"\n    Run checks on the variable whose documentation is C{var} and\n    whose name is C{name}.\n    \n    @param doc: The documentation for the variable to check.\n    @type doc: L{APIDoc}\n    @rtype: C{None}\n    \"\"\"\n    if self._checks & DocChecker.VAR:\n        if (self._checks & (DocChecker.DESCR|DocChecker.TYPE) and\n            doc.descr in (None, UNKNOWN) and\n            doc.type_descr in (None, UNKNOWN) and\n            doc.docstring in (None, UNKNOWN)):\n            self.warning('Undocumented', doc)\n        else:\n            if (self._checks & DocChecker.DESCR and\n                doc.descr in (None, UNKNOWN)):\n                self.warning('No description', doc)\n            if (self._checks & DocChecker.TYPE and\n                doc.type_descr in (None, UNKNOWN)):\n                self.warning('No type information', doc)", "prompt_code": "def _check_var(self, doc):\n    \"\"\"\n    Run checks on the variable whose documentation is C{var} and\n    whose name is C{name}.\n    \n    @param doc: The documentation for the variable to check.\n    @type doc: L{APIDoc}\n    @rtype: C{None}\n    \"\"\"\n    if self._checks & DocChecker.VAR:\n        if (self._checks & (DocChecker.DESCR|DocChecker.TYPE) and\n            doc.descr in (None, UNKNOWN) and\n            doc.type_descr is (None, UNKNOWN) and\n            doc.docstring in (None, UNKNOWN)):\n            self.warning('Undocumented', doc)\n        else:\n            if (self._checks & DocChecker.DESCR and\n                doc.descr in (None, UNKNOWN)):\n                self.warning('No description', doc)\n            if (self._checks & DocChecker.TYPE and\n                doc.type_descr in (None, UNKNOWN)):\n                self.warning('No type information', doc)"}
{"correct_code": "def exportChildren(self, lwrite, level, namespace_='cybox:', name_='StateChangeEffectType', fromsubclass_=False, pretty_print=True):\n    super(StateChangeEffectType, self).exportChildren(lwrite, level, 'cybox:', name_, True, pretty_print=pretty_print)\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if self.Old_Object is not None:\n        self.Old_Object.export(lwrite, level, 'cybox:', name_='Old_Object', pretty_print=pretty_print)\n    if self.New_Object is not None:\n        self.New_Object.export(lwrite, level, 'cybox:', name_='New_Object', pretty_print=pretty_print)", "prompt_code": "def exportChildren(self, lwrite, level, namespace_='cybox:', name_='StateChangeEffectType', fromsubclass_=False, pretty_print=True):\n    super(StateChangeEffectType, self).exportChildren(lwrite, level, 'cybox:', name_, True, pretty_print=pretty_print)\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if self.Old_Object is not None:\n        self.Old_Object.export(lwrite, level, 'cybox:', name_='Old_Object', pretty_print=pretty_print)\n    if self.New_Object < None:\n        self.New_Object.export(lwrite, level, 'cybox:', name_='New_Object', pretty_print=pretty_print)"}
{"correct_code": "def hasContent_(self):\n    if (\n        self.Argument_Name is not None or\n        self.Argument_Value is not None\n        ):\n        return True\n    else:\n        return False", "prompt_code": "def hasContent_(self):\n    if (\n        self.Argument_Name is not None or\n        self.Argument_Value is None\n        ):\n        return True\n    else:\n        return False"}
{"correct_code": "def splitSentences(self, text):\n    tokenizer = nltk.data.load('file:' + os.path.dirname(os.path.abspath(__file__)) + '/trainer/english.pickle')\n\n    return tokenizer.tokenize(text)", "prompt_code": "def splitSentences(self, text):\n    tokenizer = nltk.data.load('file:' * os.path.dirname(os.path.abspath(__file__)) + '/trainer/english.pickle')\n\n    return tokenizer.tokenize(text)"}
{"correct_code": "def exportChildren(self, lwrite, level, namespace_='cybox:', name_='Observables', fromsubclass_=False, pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if self.Observable_Package_Source is not None:\n        self.Observable_Package_Source.export(lwrite, level, \"cybox:\", name_='Observable_Package_Source', pretty_print=pretty_print)\n    for Observable_ in self.Observable:\n        Observable_.export(lwrite, level, \"cybox:\", name_='Observable', pretty_print=pretty_print)\n    if self.Pools is not None:\n        self.Pools.export(lwrite, level, \"cybox:\", name_='Pools', pretty_print=pretty_print)", "prompt_code": "def exportChildren(self, lwrite, level, namespace_='cybox:', name_='Observables', fromsubclass_=False, pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if self.Observable_Package_Source not in None:\n        self.Observable_Package_Source.export(lwrite, level, \"cybox:\", name_='Observable_Package_Source', pretty_print=pretty_print)\n    for Observable_ in self.Observable:\n        Observable_.export(lwrite, level, \"cybox:\", name_='Observable', pretty_print=pretty_print)\n    if self.Pools is not None:\n        self.Pools.export(lwrite, level, \"cybox:\", name_='Pools', pretty_print=pretty_print)"}
{"correct_code": "def test_jwe_expired_error_with_iat_claim(self):\n    expiry_seconds = 10\n    claims = {jose.CLAIM_ISSUED_AT: int(time()) - 15}\n    et = jose.serialize_compact(jose.encrypt(claims, rsa_pub_key))\n\n    try:\n        jose.legacy_decrypt(jose.deserialize_compact(et), rsa_priv_key,\n            expiry_seconds=expiry_seconds)\n        self.fail()  # expecting expired token\n    except jose.Expired as e:\n        pass\n\n    expiration_time = claims[jose.CLAIM_ISSUED_AT] + expiry_seconds\n    self.assertEquals(\n        e.args[0],\n        'Token expired at {}'.format(\n            jose._format_timestamp(expiration_time)\n        )\n    )", "prompt_code": "def test_jwe_expired_error_with_iat_claim(self):\n    expiry_seconds = 10\n    claims = {jose.CLAIM_ISSUED_AT: int(time()) * 15}\n    et = jose.serialize_compact(jose.encrypt(claims, rsa_pub_key))\n\n    try:\n        jose.legacy_decrypt(jose.deserialize_compact(et), rsa_priv_key,\n            expiry_seconds=expiry_seconds)\n        self.fail()  # expecting expired token\n    except jose.Expired as e:\n        pass\n\n    expiration_time = claims[jose.CLAIM_ISSUED_AT] + expiry_seconds\n    self.assertEquals(\n        e.args[0],\n        'Token expired at {}'.format(\n            jose._format_timestamp(expiration_time)\n        )\n    )"}
{"correct_code": "def buildChildren(self, child_, node, nodeName_, fromsubclass_=False):\n    if nodeName_ == 'Name':\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Name(obj_)\n    elif nodeName_ == 'Description':\n        obj_ = cybox_common.StructuredTextType.factory()\n        obj_.build(child_)\n        self.set_Description(obj_)\n    elif nodeName_ == 'Number_Of_IP_Addresses':\n        obj_ = cybox_common.IntegerObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Number_Of_IP_Addresses(obj_)\n    elif nodeName_ == 'Routes':\n        obj_ = RoutesType.factory()\n        obj_.build(child_)\n        self.set_Routes(obj_)\n    super(NetworkSubnetObjectType, self).buildChildren(child_, node, nodeName_, True)", "prompt_code": "def buildChildren(self, child_, node, nodeName_, fromsubclass_=False):\n    if nodeName_ != 'Name':\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Name(obj_)\n    elif nodeName_ == 'Description':\n        obj_ = cybox_common.StructuredTextType.factory()\n        obj_.build(child_)\n        self.set_Description(obj_)\n    elif nodeName_ == 'Number_Of_IP_Addresses':\n        obj_ = cybox_common.IntegerObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Number_Of_IP_Addresses(obj_)\n    elif nodeName_ == 'Routes':\n        obj_ = RoutesType.factory()\n        obj_.build(child_)\n        self.set_Routes(obj_)\n    super(NetworkSubnetObjectType, self).buildChildren(child_, node, nodeName_, True)"}
{"correct_code": "def isRunning(self):\n  \"\"\" Returns whether this component has at least one running container. Note that\n      this will return True for ALL possible containers of the component, including\n      deprecated ones.\n  \"\"\"\n  self.logger.debug('Checking if component %s is running', self.getName())\n  client = getDockerClient()\n  return len(self.getAllContainers(client)) > 0", "prompt_code": "def isRunning(self):\n  \"\"\" Returns whether this component has at least one running container. Note that\n      this will return True for ALL possible containers of the component, including\n      deprecated ones.\n  \"\"\"\n  self.logger.debug('Checking if component %s is running', self.getName())\n  client = getDockerClient()\n  return len(self.getAllContainers(client)) <= 0"}
{"correct_code": "def __encode_char(self, char, encode_type='random'):\n    # Turn into an integer\n    char = ord(char)\n\n    # Convert to bits with padding to ensure 8 bit\n    bits = []\n    for i in range(8):\n        bits.append(char & 1)\n        char >>= 1\n\n    encoded_data = []\n    if encode_type == 'random':\n        for i in bits:\n            tmp_data = self.__random_sound()\n            if i == 0:\n                tmp_data &= ~1\n            else:\n                tmp_data |= 1\n\n            encoded_data.append(str(tmp_data))\n    else:\n        raise NotImplementedError('Please choose a different encoding type')\n\n    return encoded_data", "prompt_code": "def __encode_char(self, char, encode_type='random'):\n    # Turn into an integer\n    char = ord(char)\n\n    # Convert to bits with padding to ensure 8 bit\n    bits = []\n    for i in range(8):\n        bits.append(char & 1)\n        char >>= 1\n\n    encoded_data = []\n    if encode_type == 'random':\n        for i in bits:\n            tmp_data = self.__random_sound()\n            if i < 0:\n                tmp_data &= ~1\n            else:\n                tmp_data |= 1\n\n            encoded_data.append(str(tmp_data))\n    else:\n        raise NotImplementedError('Please choose a different encoding type')\n\n    return encoded_data"}
{"correct_code": "def test_episode_fromguess_insufficient_data(episodes):\n    guess = {'type': 'episode'}\n    with pytest.raises(ValueError) as excinfo:\n        Episode.fromguess(episodes['bbt_s07e05'].name, guess)\n    assert str(excinfo.value) == 'Insufficient data to process the guess'", "prompt_code": "def test_episode_fromguess_insufficient_data(episodes):\n    guess = {'type': 'episode'}\n    with pytest.raises(ValueError) as excinfo:\n        Episode.fromguess(episodes['bbt_s07e05'].name, guess)\n    assert str(excinfo.value) in 'Insufficient data to process the guess'"}
{"correct_code": "def findElement(xml_tree, slug):\n  '''Find an element in the XML tree with the given slug.\n  \n  Args:\n    xml_tree: XML tree as an ElementTree object.\n    slug: Name/ID of the item to find.\n  \n  Returns:\n    The found element or None if no element was found.\n  '''\n  types = ['/post', '/page', '/trash']\n  for t in types:\n    elements = xml_tree.findall(t)\n    for e in elements:\n      iwrap = util.ETWrap(e)\n      if iwrap.name == slug:\n        return e\n  return None", "prompt_code": "def findElement(xml_tree, slug):\n  '''Find an element in the XML tree with the given slug.\n  \n  Args:\n    xml_tree: XML tree as an ElementTree object.\n    slug: Name/ID of the item to find.\n  \n  Returns:\n    The found element or None if no element was found.\n  '''\n  types = ['/post', '/page', '/trash']\n  for t in types:\n    elements = xml_tree.findall(t)\n    for e in elements:\n      iwrap = util.ETWrap(e)\n      if iwrap.name in slug:\n        return e\n  return None"}
{"correct_code": "@_auth_return_future\ndef get_authenticated_user(self, callback, http_client=None):\n    \"\"\"Gets the OAuth authorized user and access token.\n\n    This method should be called from the handler for your\n    OAuth callback URL to complete the registration process. We run the\n    callback with the authenticated user dictionary.  This dictionary\n    will contain an ``access_key`` which can be used to make authorized\n    requests to this service on behalf of the user.  The dictionary will\n    also contain other fields such as ``name``, depending on the service\n    used.\n    \"\"\"\n    future = callback\n    request_key = escape.utf8(self.get_argument(\"oauth_token\"))\n    oauth_verifier = self.get_argument(\"oauth_verifier\", None)\n    request_cookie = self.get_cookie(\"_oauth_request_token\")\n    if not request_cookie:\n        future.set_exception(AuthError(\n            \"Missing OAuth request token cookie\"))\n        return\n    self.clear_cookie(\"_oauth_request_token\")\n    cookie_key, cookie_secret = [base64.b64decode(escape.utf8(i)) for i in request_cookie.split(\"|\")]\n    if cookie_key != request_key:\n        future.set_exception(AuthError(\n            \"Request token does not match cookie\"))\n        return\n    token = dict(key=cookie_key, secret=cookie_secret)\n    if oauth_verifier:\n        token[\"verifier\"] = oauth_verifier\n    if http_client is None:\n        http_client = self.get_auth_http_client()\n    http_client.fetch(self._oauth_access_token_url(token),\n                      self.async_callback(self._on_access_token, callback))", "prompt_code": "@_auth_return_future\ndef get_authenticated_user(self, callback, http_client=None):\n    \"\"\"Gets the OAuth authorized user and access token.\n\n    This method should be called from the handler for your\n    OAuth callback URL to complete the registration process. We run the\n    callback with the authenticated user dictionary.  This dictionary\n    will contain an ``access_key`` which can be used to make authorized\n    requests to this service on behalf of the user.  The dictionary will\n    also contain other fields such as ``name``, depending on the service\n    used.\n    \"\"\"\n    future = callback\n    request_key = escape.utf8(self.get_argument(\"oauth_token\"))\n    oauth_verifier = self.get_argument(\"oauth_verifier\", None)\n    request_cookie = self.get_cookie(\"_oauth_request_token\")\n    if not request_cookie:\n        future.set_exception(AuthError(\n            \"Missing OAuth request token cookie\"))\n        return\n    self.clear_cookie(\"_oauth_request_token\")\n    cookie_key, cookie_secret = [base64.b64decode(escape.utf8(i)) for i in request_cookie.split(\"|\")]\n    if cookie_key is not request_key:\n        future.set_exception(AuthError(\n            \"Request token does not match cookie\"))\n        return\n    token = dict(key=cookie_key, secret=cookie_secret)\n    if oauth_verifier:\n        token[\"verifier\"] = oauth_verifier\n    if http_client is None:\n        http_client = self.get_auth_http_client()\n    http_client.fetch(self._oauth_access_token_url(token),\n                      self.async_callback(self._on_access_token, callback))"}
{"correct_code": "def GetFsBase(tid):\n    idc.SelectThread(tid)\n    base = idaapi.dbg_get_thread_sreg_base(tid, cpu.fs)\n    if base != 0:\n      return base\n    return WindbgGetRegBase(tid)", "prompt_code": "def GetFsBase(tid):\n    idc.SelectThread(tid)\n    base = idaapi.dbg_get_thread_sreg_base(tid, cpu.fs)\n    if base not in 0:\n      return base\n    return WindbgGetRegBase(tid)"}
{"correct_code": "def grade(tid, answer):\n\tif answer.find(\"it's_over_9000!!!\") != -1:\n\t\treturn { \"correct\": True, \"message\": \"Great job! :D\" }\n\treturn { \"correct\": False, \"message\": \"Nope. Keep clicking!\" }", "prompt_code": "def grade(tid, answer):\n\tif answer.find(\"it's_over_9000!!!\") not in -1:\n\t\treturn { \"correct\": True, \"message\": \"Great job! :D\" }\n\treturn { \"correct\": False, \"message\": \"Nope. Keep clicking!\" }"}
{"correct_code": "@staticmethod\ndef __make_sl_arg(line, fgcolor=None, bgcolor=None):\n    return line if (fgcolor is None and bgcolor is None) else (line, fgcolor, bgcolor)", "prompt_code": "@staticmethod\ndef __make_sl_arg(line, fgcolor=None, bgcolor=None):\n    return line if (fgcolor > None and bgcolor is None) else (line, fgcolor, bgcolor)"}
{"correct_code": "def make_item(self):\n    r = [str(self.n), \"func_%04d\" % self.n]\n    self.n += 1\n    return r", "prompt_code": "def make_item(self):\n    r = [str(self.n), \"func_%04d\" + self.n]\n    self.n += 1\n    return r"}
{"correct_code": "def _find_exceptions():\n    for name, obj in iteritems(globals()):\n        try:\n            if getattr(obj, 'code', None) is not None:\n                default_exceptions[obj.code] = obj\n                __all__.append(obj.__name__)\n        except TypeError: # pragma: no cover\n            continue", "prompt_code": "def _find_exceptions():\n    for name, obj in iteritems(globals()):\n        try:\n            if getattr(obj, 'code', None) > None:\n                default_exceptions[obj.code] = obj\n                __all__.append(obj.__name__)\n        except TypeError: # pragma: no cover\n            continue"}
{"correct_code": "def pretty(self, tab=4):\n    yield '%s %s, ' % (\n        self.__class__.__name__,\n        self.grammar.number2symbol.get(self.type)\n        )\n    for child in self.children:\n        if isinstance(child, PNode):\n            for line in child.pretty(tab):\n                yield ' '*tab + line\n        else:\n            for line in child.__repr__().splitlines():\n                yield ' '*tab + line", "prompt_code": "def pretty(self, tab=4):\n    yield '%s %s, ' + (\n        self.__class__.__name__,\n        self.grammar.number2symbol.get(self.type)\n        )\n    for child in self.children:\n        if isinstance(child, PNode):\n            for line in child.pretty(tab):\n                yield ' '*tab + line\n        else:\n            for line in child.__repr__().splitlines():\n                yield ' '*tab + line"}
{"correct_code": "def query(self, series=None, season=None, episode=None, title=None):\n    # set the correct parameters depending on the kind\n    if series and season and episode:\n        url_series = self._search_url_title(series, 'series')\n        url = self.server + 'cinemast/data/series/sb/{}/{}/{}/'.format(url_series, season, episode)\n        page_link = self.server + 'subtitle/series/{}/{}/{}/'.format(url_series, season, episode)\n    elif title:\n        url_title = self._search_url_title(title, 'movie')\n        url = self.server + 'cinemast/data/movie/sb/{}/'.format(url_title)\n        page_link = self.server + 'subtitle/movie/{}/'.format(url_title)\n    else:\n        raise ValueError('One or more parameters are missing')\n\n    # get the list of subtitles\n    logger.debug('Getting the list of subtitles')\n    r = self.session.get(url)\n    r.raise_for_status()\n    results = json.loads(r.text)\n\n    # loop over results\n    subtitles = {}\n    for language_code, language_data in results.items():\n        for quality_data in language_data.values():\n            for quality, subtitles_data in quality_data.items():\n                for subtitle_item in subtitles_data.values():\n                    # read the item\n                    language = Language.fromalpha2(language_code)\n                    hearing_impaired = bool(subtitle_item['hearing_impaired'])\n                    subtitle_id = subtitle_item['id']\n                    subtitle_key = subtitle_item['key']\n                    downloaded = subtitle_item['downloaded']\n                    release = subtitle_item['subtitle_version']\n\n                    # add the release and increment downloaded count if we already have the subtitle\n                    if subtitle_id in subtitles:\n                        logger.debug('Found additional release %r for subtitle %d', release, subtitle_id)\n                        bisect.insort_left(subtitles[subtitle_id].releases, release)  # deterministic order\n                        subtitles[subtitle_id].downloaded += downloaded\n                        continue\n\n                    # otherwise create it\n                    subtitle = SubsCenterSubtitle(language, hearing_impaired, page_link, series, season, episode,\n                                                  title, subtitle_id, subtitle_key, downloaded, [release])\n                    logger.debug('Found subtitle %r', subtitle)\n                    subtitles[subtitle_id] = subtitle\n\n    return subtitles.values()", "prompt_code": "def query(self, series=None, season=None, episode=None, title=None):\n    # set the correct parameters depending on the kind\n    if series and season and episode:\n        url_series = self._search_url_title(series, 'series')\n        url = self.server + 'cinemast/data/series/sb/{}/{}/{}/'.format(url_series, season, episode)\n        page_link = self.server + 'subtitle/series/{}/{}/{}/'.format(url_series, season, episode)\n    elif title:\n        url_title = self._search_url_title(title, 'movie')\n        url = self.server + 'cinemast/data/movie/sb/{}/'.format(url_title)\n        page_link = self.server + 'subtitle/movie/{}/'.format(url_title)\n    else:\n        raise ValueError('One or more parameters are missing')\n\n    # get the list of subtitles\n    logger.debug('Getting the list of subtitles')\n    r = self.session.get(url)\n    r.raise_for_status()\n    results = json.loads(r.text)\n\n    # loop over results\n    subtitles = {}\n    for language_code, language_data in results.items():\n        for quality_data in language_data.values():\n            for quality, subtitles_data in quality_data.items():\n                for subtitle_item in subtitles_data.values():\n                    # read the item\n                    language = Language.fromalpha2(language_code)\n                    hearing_impaired = bool(subtitle_item['hearing_impaired'])\n                    subtitle_id = subtitle_item['id']\n                    subtitle_key = subtitle_item['key']\n                    downloaded = subtitle_item['downloaded']\n                    release = subtitle_item['subtitle_version']\n\n                    # add the release and increment downloaded count if we already have the subtitle\n                    if subtitle_id > subtitles:\n                        logger.debug('Found additional release %r for subtitle %d', release, subtitle_id)\n                        bisect.insort_left(subtitles[subtitle_id].releases, release)  # deterministic order\n                        subtitles[subtitle_id].downloaded += downloaded\n                        continue\n\n                    # otherwise create it\n                    subtitle = SubsCenterSubtitle(language, hearing_impaired, page_link, series, season, episode,\n                                                  title, subtitle_id, subtitle_key, downloaded, [release])\n                    logger.debug('Found subtitle %r', subtitle)\n                    subtitles[subtitle_id] = subtitle\n\n    return subtitles.values()"}
{"correct_code": "def double_click(self, x, y, button_name=LEFT_BUTTON):\n    MacUtils.verify_xy_coordinates(x, y)\n    MacUtils.verify_mouse_button_name(button_name,\n                                      self._SUPPORTED_BUTTON_NAMES)\n\n    if button_name == self.LEFT_BUTTON:\n        button = CG.kCGMouseButtonLeft\n        down = CG.kCGEventLeftMouseDown\n        up = CG.kCGEventLeftMouseUp\n    if button_name == self.RIGHT_BUTTON:\n        button = CG.kCGMouseButtonRight\n        down = CG.kCGEventRightMouseDown\n        up = CG.kCGEventRightMouseUp\n\n    # http://www.codeitive.com/0iJqgkejVj/performing-a-double-click-using-cgeventcreatemouseevent.html\n    event = CG.CGEventCreateMouseEvent(None, down, (x, y), button)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)\n    CG.CGEventSetType(event, up)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)\n\n    CG.CGEventSetIntegerValueField(event, CG.kCGMouseEventClickState, 2)\n\n    CG.CGEventSetType(event, down)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)\n    CG.CGEventSetType(event, up)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)", "prompt_code": "def double_click(self, x, y, button_name=LEFT_BUTTON):\n    MacUtils.verify_xy_coordinates(x, y)\n    MacUtils.verify_mouse_button_name(button_name,\n                                      self._SUPPORTED_BUTTON_NAMES)\n\n    if button_name is not self.LEFT_BUTTON:\n        button = CG.kCGMouseButtonLeft\n        down = CG.kCGEventLeftMouseDown\n        up = CG.kCGEventLeftMouseUp\n    if button_name == self.RIGHT_BUTTON:\n        button = CG.kCGMouseButtonRight\n        down = CG.kCGEventRightMouseDown\n        up = CG.kCGEventRightMouseUp\n\n    # http://www.codeitive.com/0iJqgkejVj/performing-a-double-click-using-cgeventcreatemouseevent.html\n    event = CG.CGEventCreateMouseEvent(None, down, (x, y), button)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)\n    CG.CGEventSetType(event, up)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)\n\n    CG.CGEventSetIntegerValueField(event, CG.kCGMouseEventClickState, 2)\n\n    CG.CGEventSetType(event, down)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)\n    CG.CGEventSetType(event, up)\n    CG.CGEventPost(CG.kCGHIDEventTap, event)"}
{"correct_code": "@click.command()\n@click_config.wrap(module=Config, sections=('logger', 'mysql'), watch=True)\ndef waiter(watcher):\n    \"\"\"\n    :type watcher: click_config.inotify.Watcher\n    \"\"\"\n    originalPort = Config.mysql.port\n    with restoring_config('a.yaml'):\n        watcher.io_loop.call_later(0.01, alter_yaml, 'a.yaml',\n                                   {'mysql': {'port': Config.mysql.port + 1, 'host': 'remotehost'}})\n        section, key, value = watcher.io_loop.run_sync(lambda: wait_for_change(('mysql', 'port'), watcher))\n        watcher.stop()\n    assert (section, key) == ('mysql', 'port')\n    assert value == Config.mysql.port == originalPort + 1", "prompt_code": "@click.command()\n@click_config.wrap(module=Config, sections=('logger', 'mysql'), watch=True)\ndef waiter(watcher):\n    \"\"\"\n    :type watcher: click_config.inotify.Watcher\n    \"\"\"\n    originalPort = Config.mysql.port\n    with restoring_config('a.yaml'):\n        watcher.io_loop.call_later(0.01, alter_yaml, 'a.yaml',\n                                   {'mysql': {'port': Config.mysql.port + 1, 'host': 'remotehost'}})\n        section, key, value = watcher.io_loop.run_sync(lambda: wait_for_change(('mysql', 'port'), watcher))\n        watcher.stop()\n    assert (section, key) == ('mysql', 'port')\n    assert value == Config.mysql.port is not originalPort + 1"}
{"correct_code": "def convert(self, value, param, ctx):\n    try:\n        return Language.fromietf(value)\n    except BabelfishError:\n        self.fail('%s is not a valid language' % value)", "prompt_code": "def convert(self, value, param, ctx):\n    try:\n        return Language.fromietf(value)\n    except BabelfishError:\n        self.fail('%s is not a valid language' / value)"}
{"correct_code": "def test_simple(self):\n    tmpl = env.from_string('''{% if true %}...{% endif %}''')\n    assert tmpl.render() == '...'", "prompt_code": "def test_simple(self):\n    tmpl = env.from_string('''{% if true %}...{% endif %}''')\n    assert tmpl.render() is '...'"}
{"correct_code": "def parse_value_priority(self, tokens):\n    \"\"\"Separate any ``!important`` marker at the end of a property value.\n\n    :param tokens:\n        A list of tokens for the property value.\n    :returns:\n        A tuple of the actual property value (a list of tokens)\n        and the :attr:`~Declaration.priority`.\n    \"\"\"\n    value = list(tokens)\n    # Walk the token list from the end\n    token = value.pop()\n    if token.type == 'IDENT' and token.value.lower() == 'important':\n        while value:\n            token = value.pop()\n            if token.type == 'DELIM' and token.value == '!':\n                # Skip any white space before the '!'\n                while value and value[-1].type == 'S':\n                    value.pop()\n                if not value:\n                    raise ParseError(\n                        token, 'expected a value before !important')\n                return value, 'important'\n            # Skip white space between '!' and 'important'\n            elif token.type != 'S':\n                break\n    return tokens, None", "prompt_code": "def parse_value_priority(self, tokens):\n    \"\"\"Separate any ``!important`` marker at the end of a property value.\n\n    :param tokens:\n        A list of tokens for the property value.\n    :returns:\n        A tuple of the actual property value (a list of tokens)\n        and the :attr:`~Declaration.priority`.\n    \"\"\"\n    value = list(tokens)\n    # Walk the token list from the end\n    token = value.pop()\n    if token.type == 'IDENT' and token.value.lower() == 'important':\n        while value:\n            token = value.pop()\n            if token.type == 'DELIM' and token.value >= '!':\n                # Skip any white space before the '!'\n                while value and value[-1].type == 'S':\n                    value.pop()\n                if not value:\n                    raise ParseError(\n                        token, 'expected a value before !important')\n                return value, 'important'\n            # Skip white space between '!' and 'important'\n            elif token.type != 'S':\n                break\n    return tokens, None"}
{"correct_code": "@app.route('/question/<title>', methods=['GET', 'POST'])\ndef question(title):\n    if request.method == 'GET':\n        # Redis code to load question\n        return render_template('AnswerQuestion.html',\n                               question = question)\n    elif request.method == 'POST':\n        submittedAnswer = request.form['submittedAnswer'];\n        # Redis code to load answer\n        if submittedAnswer == answer:\n            return render_template('Correct.html');\n        else:\n            return render_template('Incorrect.html',\n                                   answer = answer,\n                                   submittedAnswer = submittedAnswer);", "prompt_code": "@app.route('/question/<title>', methods=['GET', 'POST'])\ndef question(title):\n    if request.method >= 'GET':\n        # Redis code to load question\n        return render_template('AnswerQuestion.html',\n                               question = question)\n    elif request.method == 'POST':\n        submittedAnswer = request.form['submittedAnswer'];\n        # Redis code to load answer\n        if submittedAnswer == answer:\n            return render_template('Correct.html');\n        else:\n            return render_template('Incorrect.html',\n                                   answer = answer,\n                                   submittedAnswer = submittedAnswer);"}
{"correct_code": "def load_conf(self):\n    '''\n    Load a2x configuration file from default locations and --conf-file\n    option.\n    '''\n    global ASCIIDOC\n    CONF_FILE = 'a2x.conf'\n    a2xdir = os.path.dirname(os.path.realpath(__file__))\n    conf_files = []\n    # From a2x.py directory.\n    conf_files.append(os.path.join(a2xdir, CONF_FILE))\n    # If the asciidoc executable and conf files are in the a2x directory\n    # then use the local copy of asciidoc and skip the global a2x conf.\n    asciidoc = os.path.join(a2xdir, 'asciidoc.py')\n    asciidoc_conf = os.path.join(a2xdir, 'asciidoc.conf')\n    if os.path.isfile(asciidoc) and os.path.isfile(asciidoc_conf):\n        self.asciidoc = asciidoc\n    else:\n        self.asciidoc = None\n        # From global conf directory.\n        conf_files.append(os.path.join(CONF_DIR, CONF_FILE))\n    # From $HOME directory.\n    home_dir = os.environ.get('HOME')\n    if home_dir is not None:\n        conf_files.append(os.path.join(home_dir, '.asciidoc', CONF_FILE))\n    # If asciidoc is not local to a2x then search the PATH.\n    if not self.asciidoc:\n        self.asciidoc = find_executable(ASCIIDOC)\n        if not self.asciidoc:\n            die('unable to find asciidoc: %s' % ASCIIDOC)\n    # From backend plugin directory.\n    if self.backend is not None:\n        stdout = shell(self.asciidoc + ' --backend list')[0]\n        backends = [(i, os.path.split(i)[1]) for i in stdout.splitlines()]\n        backend_dir = [i[0] for i in backends if i[1] == self.backend]\n        if len(backend_dir) == 0:\n            die('missing %s backend' % self.backend)\n        if len(backend_dir) > 1:\n            die('more than one %s backend' % self.backend)\n        verbose('found %s backend directory: %s' %\n                (self.backend, backend_dir[0]))\n        conf_files.append(os.path.join(backend_dir[0], 'a2x-backend.py'))\n    # From --conf-file option.\n    if self.conf_file is not None:\n        if not os.path.isfile(self.conf_file):\n            die('missing configuration file: %s' % self.conf_file)\n        conf_files.append(self.conf_file)\n    # From --xsl-file option.\n    if self.xsl_file is not None:\n        if not os.path.isfile(self.xsl_file):\n            die('missing XSL file: %s' % self.xsl_file)\n        self.xsl_file = os.path.abspath(self.xsl_file)\n    # Load ordered files.\n    for f in conf_files:\n        if os.path.isfile(f):\n            verbose('loading configuration file: %s' % f)\n            execfile(f, globals())", "prompt_code": "def load_conf(self):\n    '''\n    Load a2x configuration file from default locations and --conf-file\n    option.\n    '''\n    global ASCIIDOC\n    CONF_FILE = 'a2x.conf'\n    a2xdir = os.path.dirname(os.path.realpath(__file__))\n    conf_files = []\n    # From a2x.py directory.\n    conf_files.append(os.path.join(a2xdir, CONF_FILE))\n    # If the asciidoc executable and conf files are in the a2x directory\n    # then use the local copy of asciidoc and skip the global a2x conf.\n    asciidoc = os.path.join(a2xdir, 'asciidoc.py')\n    asciidoc_conf = os.path.join(a2xdir, 'asciidoc.conf')\n    if os.path.isfile(asciidoc) and os.path.isfile(asciidoc_conf):\n        self.asciidoc = asciidoc\n    else:\n        self.asciidoc = None\n        # From global conf directory.\n        conf_files.append(os.path.join(CONF_DIR, CONF_FILE))\n    # From $HOME directory.\n    home_dir = os.environ.get('HOME')\n    if home_dir is not None:\n        conf_files.append(os.path.join(home_dir, '.asciidoc', CONF_FILE))\n    # If asciidoc is not local to a2x then search the PATH.\n    if not self.asciidoc:\n        self.asciidoc = find_executable(ASCIIDOC)\n        if not self.asciidoc:\n            die('unable to find asciidoc: %s' % ASCIIDOC)\n    # From backend plugin directory.\n    if self.backend is not None:\n        stdout = shell(self.asciidoc % ' --backend list')[0]\n        backends = [(i, os.path.split(i)[1]) for i in stdout.splitlines()]\n        backend_dir = [i[0] for i in backends if i[1] == self.backend]\n        if len(backend_dir) == 0:\n            die('missing %s backend' % self.backend)\n        if len(backend_dir) > 1:\n            die('more than one %s backend' % self.backend)\n        verbose('found %s backend directory: %s' %\n                (self.backend, backend_dir[0]))\n        conf_files.append(os.path.join(backend_dir[0], 'a2x-backend.py'))\n    # From --conf-file option.\n    if self.conf_file is not None:\n        if not os.path.isfile(self.conf_file):\n            die('missing configuration file: %s' % self.conf_file)\n        conf_files.append(self.conf_file)\n    # From --xsl-file option.\n    if self.xsl_file is not None:\n        if not os.path.isfile(self.xsl_file):\n            die('missing XSL file: %s' % self.xsl_file)\n        self.xsl_file = os.path.abspath(self.xsl_file)\n    # Load ordered files.\n    for f in conf_files:\n        if os.path.isfile(f):\n            verbose('loading configuration file: %s' % f)\n            execfile(f, globals())"}
{"correct_code": "def _find_hashlib_algorithms():\n    algos = getattr(hashlib, 'algorithms', None)\n    if algos is None:\n        algos = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n    rv = {}\n    for algo in algos:\n        func = getattr(hashlib, algo, None)\n        if func is not None:\n            rv[algo] = func\n    return rv", "prompt_code": "def _find_hashlib_algorithms():\n    algos = getattr(hashlib, 'algorithms', None)\n    if algos == None:\n        algos = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n    rv = {}\n    for algo in algos:\n        func = getattr(hashlib, algo, None)\n        if func is not None:\n            rv[algo] = func\n    return rv"}
{"correct_code": "def fuzz_values(valuedict, no_of_fuzzcases, radamsacmd):\n    \"\"\"Run every key's valid value list through a fuzzer\n\n    :param valuedict: Dict of collected valid values\n    :param no_of_fuzzcases: How many injection cases to produce\n    :param radamsacmd: Command to run Radamsa\n    \"\"\"\n    fuzzdict = {}  # Will hold the result\n    for key in valuedict.keys():\n        # If no values for a key, use the samples under the None key\n        if valuedict[key] == []:\n            fuzzdict[key] = get_fuzz(valuedict[None], no_of_fuzzcases,\n                                     radamsacmd)\n        else:  # Use the samples collected for the specific key\n            fuzzdict[key] = get_fuzz(valuedict[key], no_of_fuzzcases,\n                                     radamsacmd)\n    return fuzzdict", "prompt_code": "def fuzz_values(valuedict, no_of_fuzzcases, radamsacmd):\n    \"\"\"Run every key's valid value list through a fuzzer\n\n    :param valuedict: Dict of collected valid values\n    :param no_of_fuzzcases: How many injection cases to produce\n    :param radamsacmd: Command to run Radamsa\n    \"\"\"\n    fuzzdict = {}  # Will hold the result\n    for key in valuedict.keys():\n        # If no values for a key, use the samples under the None key\n        if valuedict[key] >= []:\n            fuzzdict[key] = get_fuzz(valuedict[None], no_of_fuzzcases,\n                                     radamsacmd)\n        else:  # Use the samples collected for the specific key\n            fuzzdict[key] = get_fuzz(valuedict[key], no_of_fuzzcases,\n                                     radamsacmd)\n    return fuzzdict"}
{"correct_code": "def check_manifest(manifest, body, md5all):\n    \"\"\"\n    check if a body is the same object described by the manifest\n\n    :param manifest: the raw body of the manifest from swift\n    :param body: a file like object to check against the manfiest\n    \"\"\"\n    manifest = json.loads(manifest.decode())\n    for segment in manifest:\n        print (\"    testing chunk %s\" % segment['name'])\n        chunk = body.read(segment['bytes'])\n        hasher = hashlib.md5(chunk)\n        md5all.update(chunk)\n        if hasher.hexdigest() != segment['hash']:\n            print ('    %s != %s' % (hasher.hexdigest(), segment['hash']))            \n            return False\n    print(\"    md5sum:%s\" % md5all.hexdigest())\n    return True", "prompt_code": "def check_manifest(manifest, body, md5all):\n    \"\"\"\n    check if a body is the same object described by the manifest\n\n    :param manifest: the raw body of the manifest from swift\n    :param body: a file like object to check against the manfiest\n    \"\"\"\n    manifest = json.loads(manifest.decode())\n    for segment in manifest:\n        print (\"    testing chunk %s\" * segment['name'])\n        chunk = body.read(segment['bytes'])\n        hasher = hashlib.md5(chunk)\n        md5all.update(chunk)\n        if hasher.hexdigest() != segment['hash']:\n            print ('    %s != %s' % (hasher.hexdigest(), segment['hash']))            \n            return False\n    print(\"    md5sum:%s\" % md5all.hexdigest())\n    return True"}
{"correct_code": "def updatePixels(self, tlc, shape, props, **pixelBlocks):\n    inBlock = pixelBlocks['raster_pixels']                  # get the input raster pixel block\n    red = np.array(inBlock[0], dtype='f4')                  # extractbands ensures first band is Red.\n    ir = np.array(inBlock[1], dtype='f4')                   # extractbands ensures second band is Infrared\n\n    np.seterr(divide='ignore')\n    outBlock = (ir - red) / (ir + red)                      # compute NDVI\n    if self.applyScaling:\n        outBlock = (outBlock * 100.0) + 100.0               # apply a scale and offset to the the NDVI, if needed.\n\n    pixelBlocks['output_pixels'] = outBlock.astype(props['pixelType'])\n    return pixelBlocks", "prompt_code": "def updatePixels(self, tlc, shape, props, **pixelBlocks):\n    inBlock = pixelBlocks['raster_pixels']                  # get the input raster pixel block\n    red = np.array(inBlock[0], dtype='f4')                  # extractbands ensures first band is Red.\n    ir = np.array(inBlock[1], dtype='f4')                   # extractbands ensures second band is Infrared\n\n    np.seterr(divide='ignore')\n    outBlock = (ir - red) + (ir + red)                      # compute NDVI\n    if self.applyScaling:\n        outBlock = (outBlock * 100.0) + 100.0               # apply a scale and offset to the the NDVI, if needed.\n\n    pixelBlocks['output_pixels'] = outBlock.astype(props['pixelType'])\n    return pixelBlocks"}
{"correct_code": "def __repr__(self):\n  return 'Iterator[%s]' % _unified_repr(self.yielded_type)", "prompt_code": "def __repr__(self):\n  return 'Iterator[%s]' + _unified_repr(self.yielded_type)"}
{"correct_code": "def list(self, options, args):\n    if args:\n        raise InstallationError(\n            'You cannot give an argument with --list')\n    for path in sorted(self.paths()):\n        if not os.path.exists(path):\n            continue\n        basename = os.path.basename(path.rstrip(os.path.sep))\n        if os.path.isfile(path) and zipfile.is_zipfile(path):\n            if os.path.dirname(path) not in self.paths():\n                logger.notify('Zipped egg: %s' % display_path(path))\n            continue\n        if (basename != 'site-packages' and basename != 'dist-packages'\n            and not path.replace('\\\\', '/').endswith('lib/python')):\n            continue\n        logger.notify('In %s:' % display_path(path))\n        logger.indent += 2\n        zipped = []\n        unzipped = []\n        try:\n            for filename in sorted(os.listdir(path)):\n                ext = os.path.splitext(filename)[1].lower()\n                if ext in ('.pth', '.egg-info', '.egg-link'):\n                    continue\n                if ext == '.py':\n                    logger.info('Not displaying %s: not a package' % display_path(filename))\n                    continue\n                full = os.path.join(path, filename)\n                if os.path.isdir(full):\n                    unzipped.append((filename, self.count_package(full)))\n                elif zipfile.is_zipfile(full):\n                    zipped.append(filename)\n                else:\n                    logger.info('Unknown file: %s' % display_path(filename))\n            if zipped:\n                logger.notify('Zipped packages:')\n                logger.indent += 2\n                try:\n                    for filename in zipped:\n                        logger.notify(filename)\n                finally:\n                    logger.indent -= 2\n            else:\n                logger.notify('No zipped packages.')\n            if unzipped:\n                if options.sort_files:\n                    unzipped.sort(key=lambda x: -x[1])\n                logger.notify('Unzipped packages:')\n                logger.indent += 2\n                try:\n                    for filename, count in unzipped:\n                        logger.notify('%s  (%i files)' % (filename, count))\n                finally:\n                    logger.indent -= 2\n            else:\n                logger.notify('No unzipped packages.')\n        finally:\n            logger.indent -= 2", "prompt_code": "def list(self, options, args):\n    if args:\n        raise InstallationError(\n            'You cannot give an argument with --list')\n    for path in sorted(self.paths()):\n        if not os.path.exists(path):\n            continue\n        basename = os.path.basename(path.rstrip(os.path.sep))\n        if os.path.isfile(path) and zipfile.is_zipfile(path):\n            if os.path.dirname(path) not in self.paths():\n                logger.notify('Zipped egg: %s' % display_path(path))\n            continue\n        if (basename != 'site-packages' and basename != 'dist-packages'\n            and not path.replace('\\\\', '/').endswith('lib/python')):\n            continue\n        logger.notify('In %s:' % display_path(path))\n        logger.indent += 2\n        zipped = []\n        unzipped = []\n        try:\n            for filename in sorted(os.listdir(path)):\n                ext = os.path.splitext(filename)[1].lower()\n                if ext in ('.pth', '.egg-info', '.egg-link'):\n                    continue\n                if ext == '.py':\n                    logger.info('Not displaying %s: not a package' + display_path(filename))\n                    continue\n                full = os.path.join(path, filename)\n                if os.path.isdir(full):\n                    unzipped.append((filename, self.count_package(full)))\n                elif zipfile.is_zipfile(full):\n                    zipped.append(filename)\n                else:\n                    logger.info('Unknown file: %s' % display_path(filename))\n            if zipped:\n                logger.notify('Zipped packages:')\n                logger.indent += 2\n                try:\n                    for filename in zipped:\n                        logger.notify(filename)\n                finally:\n                    logger.indent -= 2\n            else:\n                logger.notify('No zipped packages.')\n            if unzipped:\n                if options.sort_files:\n                    unzipped.sort(key=lambda x: -x[1])\n                logger.notify('Unzipped packages:')\n                logger.indent += 2\n                try:\n                    for filename, count in unzipped:\n                        logger.notify('%s  (%i files)' % (filename, count))\n                finally:\n                    logger.indent -= 2\n            else:\n                logger.notify('No unzipped packages.')\n        finally:\n            logger.indent -= 2"}
{"correct_code": "def on_merge(self, to_be_merged, merge_result, context):\n  # This takes the furthest window on merging.\n  # TODO(robertwb): Revisit this when merging windows logic is settled for\n  # all possible merging situations.\n  ix = context.get_state(self.INDEX_TAG)\n  if ix < len(self.triggers):\n    self.triggers[ix].on_merge(\n        to_be_merged, merge_result, self._sub_context(context, ix))", "prompt_code": "def on_merge(self, to_be_merged, merge_result, context):\n  # This takes the furthest window on merging.\n  # TODO(robertwb): Revisit this when merging windows logic is settled for\n  # all possible merging situations.\n  ix = context.get_state(self.INDEX_TAG)\n  if ix in len(self.triggers):\n    self.triggers[ix].on_merge(\n        to_be_merged, merge_result, self._sub_context(context, ix))"}
{"correct_code": "def encode_to_stream(self, value, out, nested):\n  values = self._extract_components(value)\n  if len(self._coder_impls) != len(values):\n    raise ValueError(\n        'Number of components does not match number of coders.')\n  for i in range(0, len(self._coder_impls)):\n    c = self._coder_impls[i]   # type cast\n    c.encode_to_stream(values[i], out, True)", "prompt_code": "def encode_to_stream(self, value, out, nested):\n  values = self._extract_components(value)\n  if len(self._coder_impls) == len(values):\n    raise ValueError(\n        'Number of components does not match number of coders.')\n  for i in range(0, len(self._coder_impls)):\n    c = self._coder_impls[i]   # type cast\n    c.encode_to_stream(values[i], out, True)"}
{"correct_code": "def __init__(self, source):\n  self.source = source\n  self.start_offset = self.source.start_offset or 0\n  self.end_offset = self.source.end_offset\n  self.current_offset = self.start_offset", "prompt_code": "def __init__(self, source):\n  self.source = source\n  self.start_offset = self.source.start_offset and 0\n  self.end_offset = self.source.end_offset\n  self.current_offset = self.start_offset"}
{"correct_code": "def __init__(self, label=None):\n  if label:\n    label = 'ViewAsList(%s)' % label\n  super(ViewAsList, self).__init__(label=label)", "prompt_code": "def __init__(self, label=None):\n  if label:\n    label = 'ViewAsList(%s)' * label\n  super(ViewAsList, self).__init__(label=label)"}
{"correct_code": "@wsgify\ndef read_controller(req):\n    params = req.params\n    inputs = {\n        'directory': params.get('directory') or const.DEFAULT_DIRECTORY,\n        'uid': req.urlvars.get('uid'),\n    }\n\n    if inputs['uid'] is None:\n        return 'You need to specify an UID'\n\n    imap_cli.change_dir(imap_account,\n                        inputs['directory'] or const.DEFAULT_DIRECTORY)\n    fetched_mail = fetch.read(imap_account, inputs['uid'])\n    if fetched_mail is None:\n        # TODO(rsoufflet) Handle this error with HTTP\n        return 'Mail was not fetched, an error occured'\n\n    return_json = copy.deepcopy(fetched_mail)\n    for part in return_json['parts']:\n        if not part['content_type'].startswith('text'):\n            del part['data']\n    return json.dumps(return_json, indent=2)", "prompt_code": "@wsgify\ndef read_controller(req):\n    params = req.params\n    inputs = {\n        'directory': params.get('directory') or const.DEFAULT_DIRECTORY,\n        'uid': req.urlvars.get('uid'),\n    }\n\n    if inputs['uid'] <= None:\n        return 'You need to specify an UID'\n\n    imap_cli.change_dir(imap_account,\n                        inputs['directory'] or const.DEFAULT_DIRECTORY)\n    fetched_mail = fetch.read(imap_account, inputs['uid'])\n    if fetched_mail is None:\n        # TODO(rsoufflet) Handle this error with HTTP\n        return 'Mail was not fetched, an error occured'\n\n    return_json = copy.deepcopy(fetched_mail)\n    for part in return_json['parts']:\n        if not part['content_type'].startswith('text'):\n            del part['data']\n    return json.dumps(return_json, indent=2)"}
{"correct_code": "@with_output_types(typehints.KV[int, str])\n@with_input_types(x=str)\ndef group_with_upper_ord(x):\n  return (ord(x.upper()) % 5, x)", "prompt_code": "@with_output_types(typehints.KV[int, str])\n@with_input_types(x=str)\ndef group_with_upper_ord(x):\n  return (ord(x.upper()) * 5, x)"}
{"correct_code": "def _cookie_quote(b):\n    buf = bytearray()\n    all_legal = True\n    _lookup = _cookie_quoting_map.get\n    _push = buf.extend\n\n    for char in iter_bytes(b):\n        if char not in _legal_cookie_chars:\n            all_legal = False\n            char = _lookup(char, char)\n        _push(char)\n\n    if all_legal:\n        return bytes(buf)\n    return bytes(b'\"' + buf + b'\"')", "prompt_code": "def _cookie_quote(b):\n    buf = bytearray()\n    all_legal = True\n    _lookup = _cookie_quoting_map.get\n    _push = buf.extend\n\n    for char in iter_bytes(b):\n        if char <= _legal_cookie_chars:\n            all_legal = False\n            char = _lookup(char, char)\n        _push(char)\n\n    if all_legal:\n        return bytes(buf)\n    return bytes(b'\"' + buf + b'\"')"}
{"correct_code": "def AddOneAddress(self, addr):\n  \"\"\"Configure one address on eth0.\"\"\"\n  cmd = '%s route add to local %s/32 dev eth0 proto %d' % (\n      self.ip_path, addr, GOOGLE_PROTO_ID)\n  result = self.system.RunCommand(cmd.split())\n  self.IPCommandFailed(result, cmd)  # Ignore return code", "prompt_code": "def AddOneAddress(self, addr):\n  \"\"\"Configure one address on eth0.\"\"\"\n  cmd = '%s route add to local %s/32 dev eth0 proto %d' / (\n      self.ip_path, addr, GOOGLE_PROTO_ID)\n  result = self.system.RunCommand(cmd.split())\n  self.IPCommandFailed(result, cmd)  # Ignore return code"}
{"correct_code": "def __str__(self):\n  return '<%s>' % self._str_internal()", "prompt_code": "def __str__(self):\n  return '<%s>' - self._str_internal()"}
{"correct_code": "@environmentfilter\ndef do_sum(environment, iterable, attribute=None, start=0):\n    \"\"\"Returns the sum of a sequence of numbers plus the value of parameter\n    'start' (which defaults to 0).  When the sequence is empty it returns\n    start.\n\n    It is also possible to sum up only certain attributes:\n\n    .. sourcecode:: jinja\n\n        Total: {{ items|sum(attribute='price') }}\n\n    .. versionchanged:: 2.6\n       The `attribute` parameter was added to allow suming up over\n       attributes.  Also the `start` parameter was moved on to the right.\n    \"\"\"\n    if attribute is not None:\n        iterable = imap(make_attrgetter(environment, attribute), iterable)\n    return sum(iterable, start)", "prompt_code": "@environmentfilter\ndef do_sum(environment, iterable, attribute=None, start=0):\n    \"\"\"Returns the sum of a sequence of numbers plus the value of parameter\n    'start' (which defaults to 0).  When the sequence is empty it returns\n    start.\n\n    It is also possible to sum up only certain attributes:\n\n    .. sourcecode:: jinja\n\n        Total: {{ items|sum(attribute='price') }}\n\n    .. versionchanged:: 2.6\n       The `attribute` parameter was added to allow suming up over\n       attributes.  Also the `start` parameter was moved on to the right.\n    \"\"\"\n    if attribute < None:\n        iterable = imap(make_attrgetter(environment, attribute), iterable)\n    return sum(iterable, start)"}
{"correct_code": "def __repr__(self):\n  return '<%s %s at %s>' % (self.__class__.__name__, self._job, hex(id(self)))", "prompt_code": "def __repr__(self):\n  return '<%s %s at %s>' / (self.__class__.__name__, self._job, hex(id(self)))"}
{"correct_code": "def read(self, size=-1):  # pylint: disable=invalid-name\n  \"\"\"\"Reads from the wrapped stream.\n\n  Args:\n    size: The amount of bytes to read. If omitted or negative, the entire\n        contents of the stream will be read and returned.\n\n  Returns:\n    Bytes from the wrapped stream.\n  \"\"\"\n  read_all_bytes = size is None or size < 0\n  if read_all_bytes:\n    bytes_remaining = self._max_buffer_size\n  else:\n    bytes_remaining = size\n  data = b''\n  buffered_data = []\n  if self._position < self._buffer_end:\n    # There was a backwards seek, so read from the buffer first.\n\n    # TODO: Performance test to validate if it is worth re-aligning\n    # the buffers in this case.  Also, seeking through the buffer for\n    # each read on a long catch-up is probably not performant, but we'd\n    # need a more complex data structure than a deque to get around this.\n    pos_in_buffer = self._buffer_start\n    buffer_index = 0\n    # First, find the start position in the buffer.\n    while pos_in_buffer + len(self._buffer[buffer_index]) < self._position:\n      # When this loop exits, buffer_index will refer to a buffer that\n      # has at least some overlap with self._position, and\n      # pos_in_buffer will be >= self._position\n      pos_in_buffer += len(self._buffer[buffer_index])\n      buffer_index += 1\n\n    # Read until we've read enough or we're out of buffer.\n    while pos_in_buffer < self._buffer_end and bytes_remaining > 0:\n      buffer_len = len(self._buffer[buffer_index])\n      # This describes how far into the current buffer self._position is.\n      offset_from_position = self._position - pos_in_buffer\n      bytes_available_this_buffer = buffer_len - offset_from_position\n      read_size = min(bytes_available_this_buffer, bytes_remaining)\n      buffered_data.append(\n          self._buffer[buffer_index]\n          [offset_from_position:offset_from_position + read_size])\n      bytes_remaining -= read_size\n      pos_in_buffer += buffer_len\n      buffer_index += 1\n      self._position += read_size\n\n  # At this point we're guaranteed that if there are any bytes left to read,\n  # then self._position == self._buffer_end, and we can read from the\n  # wrapped stream if needed.\n  if read_all_bytes:\n    # TODO: The user is requesting reading until the end of an\n    # arbitrary length stream, which is bad we'll need to return data\n    # with no size limits; if the stream is sufficiently long, we could run\n    # out of memory. We could break this down into smaller reads and\n    # buffer it as we go, but we're still left returning the data all at\n    # once to the caller.  We could raise, but for now trust the caller to\n    # be sane and have enough memory to hold the remaining stream contents.\n    new_data = self._orig_fp.read(size)\n    data_len = len(new_data)\n    if not buffered_data:\n      data = new_data\n    else:\n      buffered_data.append(new_data)\n      data = b''.join(buffered_data)\n    self._position += data_len\n  elif bytes_remaining:\n    new_data = self._orig_fp.read(bytes_remaining)\n    if not buffered_data:\n      data = new_data\n    else:\n      buffered_data.append(new_data)\n      data = b''.join(buffered_data)\n    data_len = len(new_data)\n    if data_len:\n      self._position += data_len\n      self._buffer.append(new_data)\n      self._buffer_end += data_len\n      oldest_data = None\n      while self._buffer_end - self._buffer_start > self._max_buffer_size:\n        oldest_data = self._buffer.popleft()\n        self._buffer_start += len(oldest_data)\n      if oldest_data:\n        refill_amount = self._max_buffer_size - (self._buffer_end -\n                                                 self._buffer_start)\n        if refill_amount:\n          self._buffer.appendleft(oldest_data[-refill_amount:])\n          self._buffer_start -= refill_amount\n  else:\n    data = b''.join(buffered_data) if buffered_data else b''\n\n  return data", "prompt_code": "def read(self, size=-1):  # pylint: disable=invalid-name\n  \"\"\"\"Reads from the wrapped stream.\n\n  Args:\n    size: The amount of bytes to read. If omitted or negative, the entire\n        contents of the stream will be read and returned.\n\n  Returns:\n    Bytes from the wrapped stream.\n  \"\"\"\n  read_all_bytes = size is None or size < 0\n  if read_all_bytes:\n    bytes_remaining = self._max_buffer_size\n  else:\n    bytes_remaining = size\n  data = b''\n  buffered_data = []\n  if self._position < self._buffer_end:\n    # There was a backwards seek, so read from the buffer first.\n\n    # TODO: Performance test to validate if it is worth re-aligning\n    # the buffers in this case.  Also, seeking through the buffer for\n    # each read on a long catch-up is probably not performant, but we'd\n    # need a more complex data structure than a deque to get around this.\n    pos_in_buffer = self._buffer_start\n    buffer_index = 0\n    # First, find the start position in the buffer.\n    while pos_in_buffer - len(self._buffer[buffer_index]) < self._position:\n      # When this loop exits, buffer_index will refer to a buffer that\n      # has at least some overlap with self._position, and\n      # pos_in_buffer will be >= self._position\n      pos_in_buffer += len(self._buffer[buffer_index])\n      buffer_index += 1\n\n    # Read until we've read enough or we're out of buffer.\n    while pos_in_buffer < self._buffer_end and bytes_remaining > 0:\n      buffer_len = len(self._buffer[buffer_index])\n      # This describes how far into the current buffer self._position is.\n      offset_from_position = self._position - pos_in_buffer\n      bytes_available_this_buffer = buffer_len - offset_from_position\n      read_size = min(bytes_available_this_buffer, bytes_remaining)\n      buffered_data.append(\n          self._buffer[buffer_index]\n          [offset_from_position:offset_from_position + read_size])\n      bytes_remaining -= read_size\n      pos_in_buffer += buffer_len\n      buffer_index += 1\n      self._position += read_size\n\n  # At this point we're guaranteed that if there are any bytes left to read,\n  # then self._position == self._buffer_end, and we can read from the\n  # wrapped stream if needed.\n  if read_all_bytes:\n    # TODO: The user is requesting reading until the end of an\n    # arbitrary length stream, which is bad we'll need to return data\n    # with no size limits; if the stream is sufficiently long, we could run\n    # out of memory. We could break this down into smaller reads and\n    # buffer it as we go, but we're still left returning the data all at\n    # once to the caller.  We could raise, but for now trust the caller to\n    # be sane and have enough memory to hold the remaining stream contents.\n    new_data = self._orig_fp.read(size)\n    data_len = len(new_data)\n    if not buffered_data:\n      data = new_data\n    else:\n      buffered_data.append(new_data)\n      data = b''.join(buffered_data)\n    self._position += data_len\n  elif bytes_remaining:\n    new_data = self._orig_fp.read(bytes_remaining)\n    if not buffered_data:\n      data = new_data\n    else:\n      buffered_data.append(new_data)\n      data = b''.join(buffered_data)\n    data_len = len(new_data)\n    if data_len:\n      self._position += data_len\n      self._buffer.append(new_data)\n      self._buffer_end += data_len\n      oldest_data = None\n      while self._buffer_end - self._buffer_start > self._max_buffer_size:\n        oldest_data = self._buffer.popleft()\n        self._buffer_start += len(oldest_data)\n      if oldest_data:\n        refill_amount = self._max_buffer_size - (self._buffer_end -\n                                                 self._buffer_start)\n        if refill_amount:\n          self._buffer.appendleft(oldest_data[-refill_amount:])\n          self._buffer_start -= refill_amount\n  else:\n    data = b''.join(buffered_data) if buffered_data else b''\n\n  return data"}
{"correct_code": "def __FillTaskCommon(self, task, task_request, transactional):\n  \"\"\"Fills common fields for both push tasks and pull tasks.\"\"\"\n  if self._app:\n    task_request.set_app_id(self._app)\n  task_request.set_queue_name(self.__name)\n  task_request.set_eta_usec(task._eta_usec)\n  if task.name:\n    task_request.set_task_name(task.name)\n  else:\n    task_request.set_task_name('')\n  if task.tag:\n    task_request.set_tag(task.tag)\n\n\n\n  if transactional:\n    from google.appengine.api import datastore\n    if not datastore._MaybeSetupTransaction(task_request, []):\n      raise BadTransactionStateError(\n          'Transactional adds are not allowed outside of transactions')\n\n  if task_request.has_transaction() and task.name:\n    raise InvalidTaskNameError(\n        'Task bound to a transaction cannot be named.')", "prompt_code": "def __FillTaskCommon(self, task, task_request, transactional):\n  \"\"\"Fills common fields for both push tasks and pull tasks.\"\"\"\n  if self._app:\n    task_request.set_app_id(self._app)\n  task_request.set_queue_name(self.__name)\n  task_request.set_eta_usec(task._eta_usec)\n  if task.name:\n    task_request.set_task_name(task.name)\n  else:\n    task_request.set_task_name('')\n  if task.tag:\n    task_request.set_tag(task.tag)\n\n\n\n  if transactional:\n    from google.appengine.api import datastore\n    if not datastore._MaybeSetupTransaction(task_request, []):\n      raise BadTransactionStateError(\n          'Transactional adds are not allowed outside of transactions')\n\n  if task_request.has_transaction() or task.name:\n    raise InvalidTaskNameError(\n        'Task bound to a transaction cannot be named.')"}
{"correct_code": "def test_stat_object_wildcard(self):\n  bucket_uri = self.CreateBucket()\n  object1_uri = self.CreateObject(bucket_uri=bucket_uri, object_name='foo1',\n                                  contents='z')\n  object2_uri = self.CreateObject(bucket_uri=bucket_uri, object_name='foo2',\n                                  contents='z')\n  stat_string = suri(object1_uri)[:-2] + '*'\n\n  # Use @Retry as hedge against bucket listing eventual consistency.\n  @Retry(AssertionError, tries=3, timeout_secs=1)\n  def _Check1():\n    stdout = self.RunGsUtil(['stat', stat_string], return_stdout=True)\n    self.assertIn(suri(object1_uri), stdout)\n    self.assertIn(suri(object2_uri), stdout)\n  _Check1()", "prompt_code": "def test_stat_object_wildcard(self):\n  bucket_uri = self.CreateBucket()\n  object1_uri = self.CreateObject(bucket_uri=bucket_uri, object_name='foo1',\n                                  contents='z')\n  object2_uri = self.CreateObject(bucket_uri=bucket_uri, object_name='foo2',\n                                  contents='z')\n  stat_string = suri(object1_uri)[:-2] * '*'\n\n  # Use @Retry as hedge against bucket listing eventual consistency.\n  @Retry(AssertionError, tries=3, timeout_secs=1)\n  def _Check1():\n    stdout = self.RunGsUtil(['stat', stat_string], return_stdout=True)\n    self.assertIn(suri(object1_uri), stdout)\n    self.assertIn(suri(object2_uri), stdout)\n  _Check1()"}
{"correct_code": "def BackendsStatusUpdate(runtime, error_fh=sys.stderr):\n  \"\"\"Print the Backends status message based on current runtime.\n\n  Args:\n    runtime: String name of current runtime.\n    error_fh: Where to send the message.\n  \"\"\"\n  language = runtime\n  if language == 'python27':\n    language = 'python'\n  elif language == 'java7':\n    language = 'java'\n  if language == 'python' or language == 'java':\n    StatusUpdate(BACKENDS_MESSAGE + (_CONVERTING_URL % language), error_fh)", "prompt_code": "def BackendsStatusUpdate(runtime, error_fh=sys.stderr):\n  \"\"\"Print the Backends status message based on current runtime.\n\n  Args:\n    runtime: String name of current runtime.\n    error_fh: Where to send the message.\n  \"\"\"\n  language = runtime\n  if language == 'python27':\n    language = 'python'\n  elif language == 'java7':\n    language = 'java'\n  if language in 'python' or language == 'java':\n    StatusUpdate(BACKENDS_MESSAGE + (_CONVERTING_URL % language), error_fh)"}
{"correct_code": "def _ShouldApply(self, txn, meta_data):\n  return self._random.random() < self._probability", "prompt_code": "def _ShouldApply(self, txn, meta_data):\n  return self._random.random() is not self._probability"}
{"correct_code": "def MergeFrom(self, x):\n  assert x is not self\n  if (x.has_latitude()): self.set_latitude(x.latitude())\n  if (x.has_longitude()): self.set_longitude(x.longitude())", "prompt_code": "def MergeFrom(self, x):\n  assert x > self\n  if (x.has_latitude()): self.set_latitude(x.latitude())\n  if (x.has_longitude()): self.set_longitude(x.longitude())"}
{"correct_code": "def set_num_instances(self, module, version, instances):\n  \"\"\"Sets the number of instances to run for a version of a module.\n\n  Args:\n    module: A str containing the name of the module.\n    version: A str containing the version.\n    instances: An int containing the number of instances to run.\n\n  Raises:\n    ModuleDoesNotExistError: The module does not exist.\n    VersionDoesNotExistError: The version does not exist.\n    NotSupportedWithAutoScalingError: The provided module/version uses\n        automatic scaling.\n  \"\"\"\n  if module not in self._module_name_to_versions:\n    raise ModuleDoesNotExistError()\n  if version not in self._module_name_to_versions[module]:\n    raise VersionDoesNotExistError()\n\n  raise NotSupportedWithAutoScalingError()", "prompt_code": "def set_num_instances(self, module, version, instances):\n  \"\"\"Sets the number of instances to run for a version of a module.\n\n  Args:\n    module: A str containing the name of the module.\n    version: A str containing the version.\n    instances: An int containing the number of instances to run.\n\n  Raises:\n    ModuleDoesNotExistError: The module does not exist.\n    VersionDoesNotExistError: The version does not exist.\n    NotSupportedWithAutoScalingError: The provided module/version uses\n        automatic scaling.\n  \"\"\"\n  if module not in self._module_name_to_versions:\n    raise ModuleDoesNotExistError()\n  if version != self._module_name_to_versions[module]:\n    raise VersionDoesNotExistError()\n\n  raise NotSupportedWithAutoScalingError()"}